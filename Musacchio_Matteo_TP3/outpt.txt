🔍 Probing 1920 configurations...

🔧 Running config 1/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9824

🔧 Running config 2/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9447

🔧 Running config 3/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9108

🔧 Running config 4/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9802

🔧 Running config 5/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9376

🔧 Running config 6/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8726

🔧 Running config 7/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9611

🔧 Running config 8/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9471

🔧 Running config 9/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0147

🔧 Running config 10/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1546

🔧 Running config 11/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1106

🔧 Running config 12/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0574

🔧 Running config 13/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1135

🔧 Running config 14/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1452

🔧 Running config 15/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1067

🔧 Running config 16/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9987

🔧 Running config 17/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0957

🔧 Running config 18/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1255

🔧 Running config 19/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2131

🔧 Running config 20/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3526

🔧 Running config 21/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9105

🔧 Running config 22/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9206

🔧 Running config 23/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8797

🔧 Running config 24/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9748

🔧 Running config 25/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9358

🔧 Running config 26/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8890

🔧 Running config 27/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9456

🔧 Running config 28/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9421

🔧 Running config 29/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0125

🔧 Running config 30/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1535

🔧 Running config 31/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0195

🔧 Running config 32/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0068

🔧 Running config 33/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0990

🔧 Running config 34/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1396

🔧 Running config 35/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0961

🔧 Running config 36/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9411

🔧 Running config 37/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0658

🔧 Running config 38/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1170

🔧 Running config 39/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2085

🔧 Running config 40/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3522

🔧 Running config 41/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9217

🔧 Running config 42/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5384

🔧 Running config 43/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.7078

🔧 Running config 44/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2958

🔧 Running config 45/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3391

🔧 Running config 46/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8372

🔧 Running config 47/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4456

🔧 Running config 48/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3070

🔧 Running config 49/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2019

🔧 Running config 50/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2280

🔧 Running config 51/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.0924

🔧 Running config 52/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7475

🔧 Running config 53/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6645

🔧 Running config 54/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4874

🔧 Running config 55/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5862

🔧 Running config 56/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9168

🔧 Running config 57/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5746

🔧 Running config 58/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5399

🔧 Running config 59/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4237

🔧 Running config 60/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4488

🔧 Running config 61/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5755

🔧 Running config 62/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6113

🔧 Running config 63/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4460

🔧 Running config 64/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3543

🔧 Running config 65/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2754

🔧 Running config 66/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4790

🔧 Running config 67/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4149

🔧 Running config 68/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2774

🔧 Running config 69/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3173

🔧 Running config 70/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2226

🔧 Running config 71/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7693

🔧 Running config 72/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4439

🔧 Running config 73/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5581

🔧 Running config 74/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4611

🔧 Running config 75/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5246

🔧 Running config 76/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7734

🔧 Running config 77/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4955

🔧 Running config 78/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4570

🔧 Running config 79/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3233

🔧 Running config 80/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3873

🔧 Running config 81/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7285

🔧 Running config 82/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7188

🔧 Running config 83/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.3126

🔧 Running config 84/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2430

🔧 Running config 85/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3625

🔧 Running config 86/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.4796

🔧 Running config 87/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5828

🔧 Running config 88/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2648

🔧 Running config 89/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2211

🔧 Running config 90/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3092

🔧 Running config 91/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 92/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8077

🔧 Running config 93/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5039

🔧 Running config 94/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3561

🔧 Running config 95/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2516

🔧 Running config 96/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8812

🔧 Running config 97/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4923

🔧 Running config 98/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.3188

🔧 Running config 99/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2449

🔧 Running config 100/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0953

🔧 Running config 101/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8947

🔧 Running config 102/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4430

🔧 Running config 103/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4228

🔧 Running config 104/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0225

🔧 Running config 105/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2684

🔧 Running config 106/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.6377

🔧 Running config 107/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.1319

🔧 Running config 108/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9933

🔧 Running config 109/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9538

🔧 Running config 110/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1652

🔧 Running config 111/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 112/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5198

🔧 Running config 113/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2468

🔧 Running config 114/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2454

🔧 Running config 115/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1615

🔧 Running config 116/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8812

🔧 Running config 117/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2293

🔧 Running config 118/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9298

🔧 Running config 119/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0395

🔧 Running config 120/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2314

🔧 Running config 121/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9156

🔧 Running config 122/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8303

🔧 Running config 123/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8133

🔧 Running config 124/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7554

🔧 Running config 125/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6227

🔧 Running config 126/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 127/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7738

🔧 Running config 128/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7651

🔧 Running config 129/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7438

🔧 Running config 130/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4389

🔧 Running config 131/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9157

🔧 Running config 132/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8492

🔧 Running config 133/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7944

🔧 Running config 134/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7964

🔧 Running config 135/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6960

🔧 Running config 136/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 137/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8440

🔧 Running config 138/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7890

🔧 Running config 139/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7672

🔧 Running config 140/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6663

🔧 Running config 141/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9156

🔧 Running config 142/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8928

🔧 Running config 143/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8298

🔧 Running config 144/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7531

🔧 Running config 145/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7119

🔧 Running config 146/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 147/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6097

🔧 Running config 148/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8179

🔧 Running config 149/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4119

🔧 Running config 150/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5345

🔧 Running config 151/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9157

🔧 Running config 152/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8501

🔧 Running config 153/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8570

🔧 Running config 154/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7732

🔧 Running config 155/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6559

🔧 Running config 156/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 157/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8904

🔧 Running config 158/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7780

🔧 Running config 159/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7342

🔧 Running config 160/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6252

🔧 Running config 161/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8795

🔧 Running config 162/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7372

🔧 Running config 163/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7896

🔧 Running config 164/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6992

🔧 Running config 165/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7007

🔧 Running config 166/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7008

🔧 Running config 167/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6746

🔧 Running config 168/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6831

🔧 Running config 169/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7327

🔧 Running config 170/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8232

🔧 Running config 171/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8371

🔧 Running config 172/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8646

🔧 Running config 173/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9113

🔧 Running config 174/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8521

🔧 Running config 175/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8321

🔧 Running config 176/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8094

🔧 Running config 177/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8032

🔧 Running config 178/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8106

🔧 Running config 179/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8587

🔧 Running config 180/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9747

🔧 Running config 181/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7422

🔧 Running config 182/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7044

🔧 Running config 183/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7688

🔧 Running config 184/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6965

🔧 Running config 185/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6976

🔧 Running config 186/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6584

🔧 Running config 187/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6617

🔧 Running config 188/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7080

🔧 Running config 189/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7348

🔧 Running config 190/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8232

🔧 Running config 191/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7830

🔧 Running config 192/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8946

🔧 Running config 193/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8848

🔧 Running config 194/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8446

🔧 Running config 195/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8295

🔧 Running config 196/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7418

🔧 Running config 197/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7850

🔧 Running config 198/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8102

🔧 Running config 199/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8607

🔧 Running config 200/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9736

🔧 Running config 201/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5039

🔧 Running config 202/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6672

🔧 Running config 203/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2140

🔧 Running config 204/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2517

🔧 Running config 205/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0031

🔧 Running config 206/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5340

🔧 Running config 207/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3426

🔧 Running config 208/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0853

🔧 Running config 209/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0352

🔧 Running config 210/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8464

🔧 Running config 211/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6972

🔧 Running config 212/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5809

🔧 Running config 213/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5335

🔧 Running config 214/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1963

🔧 Running config 215/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0584

🔧 Running config 216/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7108

🔧 Running config 217/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4245

🔧 Running config 218/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2849

🔧 Running config 219/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0820

🔧 Running config 220/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0325

🔧 Running config 221/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4101

🔧 Running config 222/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2826

🔧 Running config 223/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2806

🔧 Running config 224/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2318

🔧 Running config 225/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9749

🔧 Running config 226/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1397

🔧 Running config 227/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0777

🔧 Running config 228/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0127

🔧 Running config 229/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9704

🔧 Running config 230/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8028

🔧 Running config 231/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4854

🔧 Running config 232/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4332

🔧 Running config 233/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5192

🔧 Running config 234/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2493

🔧 Running config 235/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0772

🔧 Running config 236/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1689

🔧 Running config 237/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2401

🔧 Running config 238/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3067

🔧 Running config 239/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0352

🔧 Running config 240/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0335

🔧 Running config 241/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8001

🔧 Running config 242/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6229

🔧 Running config 243/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1472

🔧 Running config 244/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1053

🔧 Running config 245/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3091

🔧 Running config 246/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.6406

🔧 Running config 247/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.1577

🔧 Running config 248/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9820

🔧 Running config 249/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0448

🔧 Running config 250/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9573

🔧 Running config 251/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7616

🔧 Running config 252/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6488

🔧 Running config 253/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2710

🔧 Running config 254/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2362

🔧 Running config 255/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1483

🔧 Running config 256/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7289

🔧 Running config 257/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2970

🔧 Running config 258/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2247

🔧 Running config 259/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0848

🔧 Running config 260/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0392

🔧 Running config 261/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8913

🔧 Running config 262/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.3225

🔧 Running config 263/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9427

🔧 Running config 264/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8965

🔧 Running config 265/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2519

🔧 Running config 266/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.2070

🔧 Running config 267/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.8609

🔧 Running config 268/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.8808

🔧 Running config 269/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.7210

🔧 Running config 270/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.8126

🔧 Running config 271/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8958

🔧 Running config 272/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6687

🔧 Running config 273/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1752

🔧 Running config 274/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9479

🔧 Running config 275/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1694

🔧 Running config 276/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.3488

🔧 Running config 277/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.1795

🔧 Running config 278/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.7348

🔧 Running config 279/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8705

🔧 Running config 280/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0060

🔧 Running config 281/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8971

🔧 Running config 282/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8912

🔧 Running config 283/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8080

🔧 Running config 284/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7229

🔧 Running config 285/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6285

🔧 Running config 286/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8807

🔧 Running config 287/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7570

🔧 Running config 288/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6500

🔧 Running config 289/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5214

🔧 Running config 290/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3792

🔧 Running config 291/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8971

🔧 Running config 292/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8153

🔧 Running config 293/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7187

🔧 Running config 294/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6314

🔧 Running config 295/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7022

🔧 Running config 296/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8807

🔧 Running config 297/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7897

🔧 Running config 298/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7201

🔧 Running config 299/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5891

🔧 Running config 300/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6025

🔧 Running config 301/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8971

🔧 Running config 302/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9080

🔧 Running config 303/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7507

🔧 Running config 304/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7803

🔧 Running config 305/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4218

🔧 Running config 306/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8807

🔧 Running config 307/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8339

🔧 Running config 308/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2325

🔧 Running config 309/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3100

🔧 Running config 310/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1984

🔧 Running config 311/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8896

🔧 Running config 312/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9081

🔧 Running config 313/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7986

🔧 Running config 314/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6203

🔧 Running config 315/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6930

🔧 Running config 316/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8878

🔧 Running config 317/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6617

🔧 Running config 318/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8045

🔧 Running config 319/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4238

🔧 Running config 320/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3589

🔧 Running config 321/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6810

🔧 Running config 322/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6872

🔧 Running config 323/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6664

🔧 Running config 324/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6386

🔧 Running config 325/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6216

🔧 Running config 326/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5251

🔧 Running config 327/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.5764

🔧 Running config 328/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.5807

🔧 Running config 329/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.5884

🔧 Running config 330/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6345

🔧 Running config 331/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6631

🔧 Running config 332/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6647

🔧 Running config 333/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6242

🔧 Running config 334/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6477

🔧 Running config 335/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6553

🔧 Running config 336/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6570

🔧 Running config 337/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6418

🔧 Running config 338/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6314

🔧 Running config 339/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6401

🔧 Running config 340/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7028

🔧 Running config 341/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5906

🔧 Running config 342/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6130

🔧 Running config 343/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6410

🔧 Running config 344/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6259

🔧 Running config 345/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6127

🔧 Running config 346/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5128

🔧 Running config 347/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.5514

🔧 Running config 348/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.5625

🔧 Running config 349/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.5896

🔧 Running config 350/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6352

🔧 Running config 351/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6786

🔧 Running config 352/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6943

🔧 Running config 353/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7289

🔧 Running config 354/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6461

🔧 Running config 355/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6599

🔧 Running config 356/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5750

🔧 Running config 357/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6132

🔧 Running config 358/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6311

🔧 Running config 359/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6415

🔧 Running config 360/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7018

🔧 Running config 361/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9549

🔧 Running config 362/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7681

🔧 Running config 363/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1800

🔧 Running config 364/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9656

🔧 Running config 365/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8688

🔧 Running config 366/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7181

🔧 Running config 367/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2085

🔧 Running config 368/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9679

🔧 Running config 369/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7556

🔧 Running config 370/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6820

🔧 Running config 371/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8784

🔧 Running config 372/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6339

🔧 Running config 373/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4143

🔧 Running config 374/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1933

🔧 Running config 375/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0822

🔧 Running config 376/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4587

🔧 Running config 377/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2348

🔧 Running config 378/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0507

🔧 Running config 379/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9948

🔧 Running config 380/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8169

🔧 Running config 381/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2451

🔧 Running config 382/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2689

🔧 Running config 383/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0377

🔧 Running config 384/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8984

🔧 Running config 385/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8321

🔧 Running config 386/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7312

🔧 Running config 387/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9301

🔧 Running config 388/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7676

🔧 Running config 389/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8210

🔧 Running config 390/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6719

🔧 Running config 391/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1226

🔧 Running config 392/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2946

🔧 Running config 393/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1285

🔧 Running config 394/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1208

🔧 Running config 395/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8375

🔧 Running config 396/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9987

🔧 Running config 397/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0308

🔧 Running config 398/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9327

🔧 Running config 399/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9550

🔧 Running config 400/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7761

🔧 Running config 401/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7509

🔧 Running config 402/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5686

🔧 Running config 403/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1198

🔧 Running config 404/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8338

🔧 Running config 405/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9399

🔧 Running config 406/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.2610

🔧 Running config 407/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2121

🔧 Running config 408/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9903

🔧 Running config 409/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0260

🔧 Running config 410/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.6759

🔧 Running config 411/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7452

🔧 Running config 412/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5685

🔧 Running config 413/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5326

🔧 Running config 414/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1871

🔧 Running config 415/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9989

🔧 Running config 416/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.3483

🔧 Running config 417/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2734

🔧 Running config 418/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9644

🔧 Running config 419/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9996

🔧 Running config 420/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0732

🔧 Running config 421/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7967

🔧 Running config 422/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6170

🔧 Running config 423/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.0001

🔧 Running config 424/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.7680

🔧 Running config 425/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.6419

🔧 Running config 426/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.3651

🔧 Running config 427/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.8028

🔧 Running config 428/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6858

🔧 Running config 429/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5354

🔧 Running config 430/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.6373

🔧 Running config 431/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8650

🔧 Running config 432/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5224

🔧 Running config 433/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.0337

🔧 Running config 434/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0556

🔧 Running config 435/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.7223

🔧 Running config 436/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.3665

🔧 Running config 437/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.8796

🔧 Running config 438/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6389

🔧 Running config 439/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8239

🔧 Running config 440/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.7551

🔧 Running config 441/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9209

🔧 Running config 442/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8319

🔧 Running config 443/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6792

🔧 Running config 444/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4284

🔧 Running config 445/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4019

🔧 Running config 446/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7758

🔧 Running config 447/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7264

🔧 Running config 448/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5398

🔧 Running config 449/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2120

🔧 Running config 450/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1736

🔧 Running config 451/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9086

🔧 Running config 452/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8358

🔧 Running config 453/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7236

🔧 Running config 454/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6419

🔧 Running config 455/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7098

🔧 Running config 456/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8945

🔧 Running config 457/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7510

🔧 Running config 458/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6114

🔧 Running config 459/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2822

🔧 Running config 460/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1477

🔧 Running config 461/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9210

🔧 Running config 462/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8536

🔧 Running config 463/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8039

🔧 Running config 464/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3953

🔧 Running config 465/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3362

🔧 Running config 466/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 467/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8181

🔧 Running config 468/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.0973

🔧 Running config 469/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8926

🔧 Running config 470/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9809

🔧 Running config 471/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9209

🔧 Running config 472/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8990

🔧 Running config 473/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7674

🔧 Running config 474/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4577

🔧 Running config 475/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2509

🔧 Running config 476/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 477/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8850

🔧 Running config 478/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1450

🔧 Running config 479/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8638

🔧 Running config 480/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1070

🔧 Running config 481/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3346

🔧 Running config 482/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1491

🔧 Running config 483/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9833

🔧 Running config 484/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0584

🔧 Running config 485/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1010

🔧 Running config 486/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1349

🔧 Running config 487/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9655

🔧 Running config 488/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9679

🔧 Running config 489/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9688

🔧 Running config 490/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1459

🔧 Running config 491/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2868

🔧 Running config 492/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2649

🔧 Running config 493/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2548

🔧 Running config 494/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2179

🔧 Running config 495/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1808

🔧 Running config 496/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2457

🔧 Running config 497/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2076

🔧 Running config 498/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2480

🔧 Running config 499/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3375

🔧 Running config 500/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4437

🔧 Running config 501/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0863

🔧 Running config 502/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0806

🔧 Running config 503/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0764

🔧 Running config 504/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0374

🔧 Running config 505/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0805

🔧 Running config 506/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9839

🔧 Running config 507/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9189

🔧 Running config 508/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0126

🔧 Running config 509/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9777

🔧 Running config 510/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1472

🔧 Running config 511/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0584

🔧 Running config 512/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1060

🔧 Running config 513/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2380

🔧 Running config 514/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1892

🔧 Running config 515/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2300

🔧 Running config 516/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1161

🔧 Running config 517/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2171

🔧 Running config 518/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2503

🔧 Running config 519/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3322

🔧 Running config 520/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4377

🔧 Running config 521/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5626

🔧 Running config 522/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2968

🔧 Running config 523/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2251

🔧 Running config 524/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3196

🔧 Running config 525/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1115

🔧 Running config 526/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4727

🔧 Running config 527/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3232

🔧 Running config 528/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2135

🔧 Running config 529/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3177

🔧 Running config 530/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1373

🔧 Running config 531/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7078

🔧 Running config 532/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5161

🔧 Running config 533/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4191

🔧 Running config 534/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5414

🔧 Running config 535/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4436

🔧 Running config 536/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8104

🔧 Running config 537/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4964

🔧 Running config 538/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5002

🔧 Running config 539/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4193

🔧 Running config 540/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4270

🔧 Running config 541/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2457

🔧 Running config 542/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4146

🔧 Running config 543/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3196

🔧 Running config 544/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3618

🔧 Running config 545/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1046

🔧 Running config 546/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1630

🔧 Running config 547/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2040

🔧 Running config 548/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1780

🔧 Running config 549/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1288

🔧 Running config 550/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1198

🔧 Running config 551/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5514

🔧 Running config 552/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5183

🔧 Running config 553/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5878

🔧 Running config 554/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4809

🔧 Running config 555/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3814

🔧 Running config 556/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4763

🔧 Running config 557/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3308

🔧 Running config 558/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3913

🔧 Running config 559/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3623

🔧 Running config 560/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4076

🔧 Running config 561/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 562/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8854

🔧 Running config 563/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8842

🔧 Running config 564/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8760

🔧 Running config 565/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8791

🔧 Running config 566/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8775

🔧 Running config 567/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8793

🔧 Running config 568/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8784

🔧 Running config 569/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8765

🔧 Running config 570/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8787

🔧 Running config 571/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 572/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8854

🔧 Running config 573/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7453

🔧 Running config 574/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8787

🔧 Running config 575/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8791

🔧 Running config 576/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8775

🔧 Running config 577/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8793

🔧 Running config 578/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5612

🔧 Running config 579/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8803

🔧 Running config 580/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9746

🔧 Running config 581/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 582/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8854

🔧 Running config 583/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8842

🔧 Running config 584/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8788

🔧 Running config 585/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8791

🔧 Running config 586/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8775

🔧 Running config 587/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6233

🔧 Running config 588/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8784

🔧 Running config 589/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8765

🔧 Running config 590/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8787

🔧 Running config 591/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8933

🔧 Running config 592/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8854

🔧 Running config 593/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8802

🔧 Running config 594/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8787

🔧 Running config 595/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3772

🔧 Running config 596/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8775

🔧 Running config 597/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8793

🔧 Running config 598/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8778

🔧 Running config 599/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8803

🔧 Running config 600/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1096

🔧 Running config 601/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9114

🔧 Running config 602/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9009

🔧 Running config 603/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8823

🔧 Running config 604/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8767

🔧 Running config 605/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8807

🔧 Running config 606/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8810

🔧 Running config 607/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8781

🔧 Running config 608/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8792

🔧 Running config 609/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8789

🔧 Running config 610/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8790

🔧 Running config 611/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9114

🔧 Running config 612/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9009

🔧 Running config 613/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8824

🔧 Running config 614/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8774

🔧 Running config 615/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8804

🔧 Running config 616/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8810

🔧 Running config 617/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8781

🔧 Running config 618/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8792

🔧 Running config 619/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8803

🔧 Running config 620/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8792

🔧 Running config 621/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9114

🔧 Running config 622/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9009

🔧 Running config 623/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8823

🔧 Running config 624/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8768

🔧 Running config 625/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8807

🔧 Running config 626/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8810

🔧 Running config 627/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8781

🔧 Running config 628/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8792

🔧 Running config 629/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8789

🔧 Running config 630/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8790

🔧 Running config 631/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9114

🔧 Running config 632/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9009

🔧 Running config 633/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8823

🔧 Running config 634/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8765

🔧 Running config 635/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8803

🔧 Running config 636/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8810

🔧 Running config 637/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8781

🔧 Running config 638/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8792

🔧 Running config 639/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8803

🔧 Running config 640/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8790

🔧 Running config 641/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0694

🔧 Running config 642/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7847

🔧 Running config 643/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7065

🔧 Running config 644/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8314

🔧 Running config 645/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7846

🔧 Running config 646/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7831

🔧 Running config 647/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6520

🔧 Running config 648/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7106

🔧 Running config 649/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7009

🔧 Running config 650/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7865

🔧 Running config 651/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9332

🔧 Running config 652/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9339

🔧 Running config 653/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9669

🔧 Running config 654/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8797

🔧 Running config 655/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9159

🔧 Running config 656/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8523

🔧 Running config 657/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8287

🔧 Running config 658/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8307

🔧 Running config 659/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8417

🔧 Running config 660/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9104

🔧 Running config 661/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8063

🔧 Running config 662/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6942

🔧 Running config 663/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6883

🔧 Running config 664/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8066

🔧 Running config 665/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7885

🔧 Running config 666/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6599

🔧 Running config 667/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6171

🔧 Running config 668/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6795

🔧 Running config 669/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6987

🔧 Running config 670/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7763

🔧 Running config 671/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8890

🔧 Running config 672/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9095

🔧 Running config 673/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9223

🔧 Running config 674/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8511

🔧 Running config 675/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9409

🔧 Running config 676/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7567

🔧 Running config 677/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8061

🔧 Running config 678/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8296

🔧 Running config 679/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8420

🔧 Running config 680/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9452

🔧 Running config 681/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4298

🔧 Running config 682/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7949

🔧 Running config 683/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3696

🔧 Running config 684/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3870

🔧 Running config 685/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1668

🔧 Running config 686/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4573

🔧 Running config 687/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4623

🔧 Running config 688/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2508

🔧 Running config 689/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1245

🔧 Running config 690/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9351

🔧 Running config 691/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7385

🔧 Running config 692/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5083

🔧 Running config 693/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3160

🔧 Running config 694/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3101

🔧 Running config 695/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1719

🔧 Running config 696/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5849

🔧 Running config 697/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2445

🔧 Running config 698/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0554

🔧 Running config 699/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3533

🔧 Running config 700/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0473

🔧 Running config 701/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2991

🔧 Running config 702/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2791

🔧 Running config 703/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2204

🔧 Running config 704/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2145

🔧 Running config 705/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0525

🔧 Running config 706/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1007

🔧 Running config 707/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1919

🔧 Running config 708/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2763

🔧 Running config 709/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0837

🔧 Running config 710/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8845

🔧 Running config 711/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2844

🔧 Running config 712/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3481

🔧 Running config 713/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3829

🔧 Running config 714/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4344

🔧 Running config 715/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1503

🔧 Running config 716/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1751

🔧 Running config 717/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4012

🔧 Running config 718/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2748

🔧 Running config 719/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1899

🔧 Running config 720/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9941

🔧 Running config 721/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9057

🔧 Running config 722/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4667

🔧 Running config 723/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7870

🔧 Running config 724/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0616

🔧 Running config 725/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9483

🔧 Running config 726/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8789

🔧 Running config 727/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2498

🔧 Running config 728/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1474

🔧 Running config 729/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9346

🔧 Running config 730/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.8477

🔧 Running config 731/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9057

🔧 Running config 732/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8822

🔧 Running config 733/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8500

🔧 Running config 734/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8785

🔧 Running config 735/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9141

🔧 Running config 736/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8789

🔧 Running config 737/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5603

🔧 Running config 738/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8068

🔧 Running config 739/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2370

🔧 Running config 740/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9247

🔧 Running config 741/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9058

🔧 Running config 742/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6220

🔧 Running config 743/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.3759

🔧 Running config 744/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8027

🔧 Running config 745/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0130

🔧 Running config 746/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8789

🔧 Running config 747/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.2715

🔧 Running config 748/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9224

🔧 Running config 749/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8033

🔧 Running config 750/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.8685

🔧 Running config 751/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9057

🔧 Running config 752/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8436

🔧 Running config 753/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8674

🔧 Running config 754/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4083

🔧 Running config 755/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9887

🔧 Running config 756/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8789

🔧 Running config 757/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5097

🔧 Running config 758/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5409

🔧 Running config 759/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9378

🔧 Running config 760/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9896

🔧 Running config 761/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9363

🔧 Running config 762/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9140

🔧 Running config 763/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8951

🔧 Running config 764/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8792

🔧 Running config 765/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8205

🔧 Running config 766/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8788

🔧 Running config 767/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8877

🔧 Running config 768/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8831

🔧 Running config 769/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8825

🔧 Running config 770/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7473

🔧 Running config 771/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9365

🔧 Running config 772/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9142

🔧 Running config 773/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8952

🔧 Running config 774/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8790

🔧 Running config 775/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8841

🔧 Running config 776/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8788

🔧 Running config 777/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8802

🔧 Running config 778/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8804

🔧 Running config 779/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8777

🔧 Running config 780/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8818

🔧 Running config 781/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9363

🔧 Running config 782/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9140

🔧 Running config 783/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6597

🔧 Running config 784/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8835

🔧 Running config 785/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8793

🔧 Running config 786/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8788

🔧 Running config 787/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8802

🔧 Running config 788/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5550

🔧 Running config 789/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8808

🔧 Running config 790/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8815

🔧 Running config 791/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9365

🔧 Running config 792/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9142

🔧 Running config 793/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8952

🔧 Running config 794/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8836

🔧 Running config 795/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8817

🔧 Running config 796/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8788

🔧 Running config 797/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8802

🔧 Running config 798/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8804

🔧 Running config 799/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8808

🔧 Running config 800/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8808

🔧 Running config 801/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7466

🔧 Running config 802/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7891

🔧 Running config 803/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7102

🔧 Running config 804/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6954

🔧 Running config 805/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6313

🔧 Running config 806/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7543

🔧 Running config 807/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6333

🔧 Running config 808/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.5742

🔧 Running config 809/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.5734

🔧 Running config 810/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.5984

🔧 Running config 811/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8573

🔧 Running config 812/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9119

🔧 Running config 813/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7570

🔧 Running config 814/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7664

🔧 Running config 815/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8164

🔧 Running config 816/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7986

🔧 Running config 817/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7334

🔧 Running config 818/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6440

🔧 Running config 819/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7047

🔧 Running config 820/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7380

🔧 Running config 821/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7266

🔧 Running config 822/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6841

🔧 Running config 823/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6724

🔧 Running config 824/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6616

🔧 Running config 825/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6343

🔧 Running config 826/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5511

🔧 Running config 827/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.5687

🔧 Running config 828/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.5784

🔧 Running config 829/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.5657

🔧 Running config 830/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.5962

🔧 Running config 831/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8510

🔧 Running config 832/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8301

🔧 Running config 833/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7258

🔧 Running config 834/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7179

🔧 Running config 835/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8059

🔧 Running config 836/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5884

🔧 Running config 837/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6833

🔧 Running config 838/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6335

🔧 Running config 839/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7076

🔧 Running config 840/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7267

🔧 Running config 841/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4778

🔧 Running config 842/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4926

🔧 Running config 843/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4563

🔧 Running config 844/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3265

🔧 Running config 845/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1471

🔧 Running config 846/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2536

🔧 Running config 847/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7004

🔧 Running config 848/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2685

🔧 Running config 849/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0168

🔧 Running config 850/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0001

🔧 Running config 851/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7088

🔧 Running config 852/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4393

🔧 Running config 853/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4081

🔧 Running config 854/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1387

🔧 Running config 855/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0950

🔧 Running config 856/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3318

🔧 Running config 857/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5786

🔧 Running config 858/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2146

🔧 Running config 859/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0046

🔧 Running config 860/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0356

🔧 Running config 861/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1356

🔧 Running config 862/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4192

🔧 Running config 863/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1597

🔧 Running config 864/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4914

🔧 Running config 865/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0928

🔧 Running config 866/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9490

🔧 Running config 867/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0244

🔧 Running config 868/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0125

🔧 Running config 869/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0623

🔧 Running config 870/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8872

🔧 Running config 871/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3564

🔧 Running config 872/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2236

🔧 Running config 873/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2434

🔧 Running config 874/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2280

🔧 Running config 875/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0654

🔧 Running config 876/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1133

🔧 Running config 877/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9657

🔧 Running config 878/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0611

🔧 Running config 879/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0182

🔧 Running config 880/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9670

🔧 Running config 881/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8957

🔧 Running config 882/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6034

🔧 Running config 883/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8208

🔧 Running config 884/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0335

🔧 Running config 885/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.8620

🔧 Running config 886/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8877

🔧 Running config 887/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4152

🔧 Running config 888/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9428

🔧 Running config 889/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.7354

🔧 Running config 890/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4339

🔧 Running config 891/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8573

🔧 Running config 892/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8832

🔧 Running config 893/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5510

🔧 Running config 894/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3187

🔧 Running config 895/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1521

🔧 Running config 896/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8438

🔧 Running config 897/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8789

🔧 Running config 898/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.3545

🔧 Running config 899/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1824

🔧 Running config 900/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0292

🔧 Running config 901/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8846

🔧 Running config 902/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7169

🔧 Running config 903/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2683

🔧 Running config 904/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8419

🔧 Running config 905/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5382

🔧 Running config 906/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8784

🔧 Running config 907/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.1701

🔧 Running config 908/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.9693

🔧 Running config 909/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.7752

🔧 Running config 910/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5340

🔧 Running config 911/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8846

🔧 Running config 912/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7339

🔧 Running config 913/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.3853

🔧 Running config 914/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2998

🔧 Running config 915/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0813

🔧 Running config 916/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8784

🔧 Running config 917/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8791

🔧 Running config 918/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1226

🔧 Running config 919/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8508

🔧 Running config 920/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.7865

🔧 Running config 921/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8979

🔧 Running config 922/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8889

🔧 Running config 923/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6527

🔧 Running config 924/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8819

🔧 Running config 925/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6756

🔧 Running config 926/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 927/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8810

🔧 Running config 928/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6838

🔧 Running config 929/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8428

🔧 Running config 930/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5436

🔧 Running config 931/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8979

🔧 Running config 932/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8889

🔧 Running config 933/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8396

🔧 Running config 934/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8778

🔧 Running config 935/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6874

🔧 Running config 936/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 937/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8809

🔧 Running config 938/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7869

🔧 Running config 939/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 940/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5543

🔧 Running config 941/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8979

🔧 Running config 942/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8889

🔧 Running config 943/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6731

🔧 Running config 944/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8293

🔧 Running config 945/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4066

🔧 Running config 946/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 947/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8809

🔧 Running config 948/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4600

🔧 Running config 949/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0774

🔧 Running config 950/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4275

🔧 Running config 951/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8979

🔧 Running config 952/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8889

🔧 Running config 953/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8904

🔧 Running config 954/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8777

🔧 Running config 955/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5328

🔧 Running config 956/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8802

🔧 Running config 957/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8809

🔧 Running config 958/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8876

🔧 Running config 959/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8784

🔧 Running config 960/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2784

🔧 Running config 961/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2309

🔧 Running config 962/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0751

🔧 Running config 963/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9450

🔧 Running config 964/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8964

🔧 Running config 965/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9634

🔧 Running config 966/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0034

🔧 Running config 967/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9115

🔧 Running config 968/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9054

🔧 Running config 969/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9010

🔧 Running config 970/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9787

🔧 Running config 971/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0329

🔧 Running config 972/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1343

🔧 Running config 973/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1805

🔧 Running config 974/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1550

🔧 Running config 975/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0841

🔧 Running config 976/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0639

🔧 Running config 977/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0031

🔧 Running config 978/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9965

🔧 Running config 979/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1442

🔧 Running config 980/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2115

🔧 Running config 981/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9857

🔧 Running config 982/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0156

🔧 Running config 983/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9961

🔧 Running config 984/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8614

🔧 Running config 985/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9623

🔧 Running config 986/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7973

🔧 Running config 987/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8118

🔧 Running config 988/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8258

🔧 Running config 989/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8894

🔧 Running config 990/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9731

🔧 Running config 991/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9873

🔧 Running config 992/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1443

🔧 Running config 993/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1028

🔧 Running config 994/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1152

🔧 Running config 995/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0999

🔧 Running config 996/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9667

🔧 Running config 997/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0242

🔧 Running config 998/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0271

🔧 Running config 999/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0918

🔧 Running config 1000/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2259

🔧 Running config 1001/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4831

🔧 Running config 1002/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4937

🔧 Running config 1003/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5285

🔧 Running config 1004/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4573

🔧 Running config 1005/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0859

🔧 Running config 1006/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4266

🔧 Running config 1007/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3272

🔧 Running config 1008/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2229

🔧 Running config 1009/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3564

🔧 Running config 1010/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0126

🔧 Running config 1011/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8890

🔧 Running config 1012/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6734

🔧 Running config 1013/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6810

🔧 Running config 1014/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2533

🔧 Running config 1015/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2788

🔧 Running config 1016/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6120

🔧 Running config 1017/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5287

🔧 Running config 1018/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3821

🔧 Running config 1019/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3197

🔧 Running config 1020/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3191

🔧 Running config 1021/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2584

🔧 Running config 1022/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4002

🔧 Running config 1023/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1138

🔧 Running config 1024/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3351

🔧 Running config 1025/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2485

🔧 Running config 1026/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0756

🔧 Running config 1027/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2774

🔧 Running config 1028/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3357

🔧 Running config 1029/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1823

🔧 Running config 1030/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1888

🔧 Running config 1031/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5409

🔧 Running config 1032/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3269

🔧 Running config 1033/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3354

🔧 Running config 1034/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3678

🔧 Running config 1035/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4596

🔧 Running config 1036/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2664

🔧 Running config 1037/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3032

🔧 Running config 1038/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2537

🔧 Running config 1039/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3308

🔧 Running config 1040/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2287

🔧 Running config 1041/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8980

🔧 Running config 1042/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6696

🔧 Running config 1043/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8778

🔧 Running config 1044/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2812

🔧 Running config 1045/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8809

🔧 Running config 1046/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8855

🔧 Running config 1047/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5371

🔧 Running config 1048/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8796

🔧 Running config 1049/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3314

🔧 Running config 1050/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8801

🔧 Running config 1051/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8980

🔧 Running config 1052/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8710

🔧 Running config 1053/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8779

🔧 Running config 1054/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5682

🔧 Running config 1055/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8790

🔧 Running config 1056/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8855

🔧 Running config 1057/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8459

🔧 Running config 1058/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8775

🔧 Running config 1059/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5976

🔧 Running config 1060/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8787

🔧 Running config 1061/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8980

🔧 Running config 1062/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7346

🔧 Running config 1063/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8779

🔧 Running config 1064/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2629

🔧 Running config 1065/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8810

🔧 Running config 1066/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8855

🔧 Running config 1067/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4259

🔧 Running config 1068/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8784

🔧 Running config 1069/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0947

🔧 Running config 1070/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8801

🔧 Running config 1071/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8980

🔧 Running config 1072/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8866

🔧 Running config 1073/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8779

🔧 Running config 1074/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4483

🔧 Running config 1075/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8783

🔧 Running config 1076/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8855

🔧 Running config 1077/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8811

🔧 Running config 1078/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8775

🔧 Running config 1079/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3030

🔧 Running config 1080/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8784

🔧 Running config 1081/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9003

🔧 Running config 1082/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9041

🔧 Running config 1083/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8928

🔧 Running config 1084/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 1085/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8086

🔧 Running config 1086/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8889

🔧 Running config 1087/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8883

🔧 Running config 1088/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8793

🔧 Running config 1089/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8800

🔧 Running config 1090/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8157

🔧 Running config 1091/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9003

🔧 Running config 1092/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9042

🔧 Running config 1093/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8900

🔧 Running config 1094/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8781

🔧 Running config 1095/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8830

🔧 Running config 1096/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8889

🔧 Running config 1097/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8884

🔧 Running config 1098/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8779

🔧 Running config 1099/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8801

🔧 Running config 1100/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8782

🔧 Running config 1101/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9003

🔧 Running config 1102/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9041

🔧 Running config 1103/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8922

🔧 Running config 1104/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 1105/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8822

🔧 Running config 1106/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8889

🔧 Running config 1107/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8883

🔧 Running config 1108/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8793

🔧 Running config 1109/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8800

🔧 Running config 1110/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8808

🔧 Running config 1111/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9003

🔧 Running config 1112/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9042

🔧 Running config 1113/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8924

🔧 Running config 1114/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8781

🔧 Running config 1115/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8831

🔧 Running config 1116/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8889

🔧 Running config 1117/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8884

🔧 Running config 1118/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8845

🔧 Running config 1119/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8801

🔧 Running config 1120/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8783

🔧 Running config 1121/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1603

🔧 Running config 1122/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9796

🔧 Running config 1123/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8996

🔧 Running config 1124/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9358

🔧 Running config 1125/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8419

🔧 Running config 1126/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9934

🔧 Running config 1127/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8337

🔧 Running config 1128/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9128

🔧 Running config 1129/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8622

🔧 Running config 1130/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7937

🔧 Running config 1131/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0485

🔧 Running config 1132/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1871

🔧 Running config 1133/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9078

🔧 Running config 1134/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9823

🔧 Running config 1135/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0184

🔧 Running config 1136/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9025

🔧 Running config 1137/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9235

🔧 Running config 1138/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8775

🔧 Running config 1139/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8640

🔧 Running config 1140/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9976

🔧 Running config 1141/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0170

🔧 Running config 1142/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9369

🔧 Running config 1143/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8589

🔧 Running config 1144/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8701

🔧 Running config 1145/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8871

🔧 Running config 1146/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9193

🔧 Running config 1147/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8497

🔧 Running config 1148/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8813

🔧 Running config 1149/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8495

🔧 Running config 1150/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7734

🔧 Running config 1151/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9209

🔧 Running config 1152/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9222

🔧 Running config 1153/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8697

🔧 Running config 1154/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9670

🔧 Running config 1155/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9607

🔧 Running config 1156/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8694

🔧 Running config 1157/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9549

🔧 Running config 1158/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8684

🔧 Running config 1159/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8974

🔧 Running config 1160/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9675

🔧 Running config 1161/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7253

🔧 Running config 1162/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2666

🔧 Running config 1163/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3363

🔧 Running config 1164/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2561

🔧 Running config 1165/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1089

🔧 Running config 1166/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3646

🔧 Running config 1167/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2499

🔧 Running config 1168/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0988

🔧 Running config 1169/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3457

🔧 Running config 1170/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2242

🔧 Running config 1171/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.0898

🔧 Running config 1172/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6542

🔧 Running config 1173/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2978

🔧 Running config 1174/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3923

🔧 Running config 1175/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3004

🔧 Running config 1176/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5795

🔧 Running config 1177/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6098

🔧 Running config 1178/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2659

🔧 Running config 1179/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1976

🔧 Running config 1180/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2228

🔧 Running config 1181/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3323

🔧 Running config 1182/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4036

🔧 Running config 1183/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1497

🔧 Running config 1184/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3692

🔧 Running config 1185/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2529

🔧 Running config 1186/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1495

🔧 Running config 1187/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0282

🔧 Running config 1188/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3489

🔧 Running config 1189/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0957

🔧 Running config 1190/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2558

🔧 Running config 1191/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6875

🔧 Running config 1192/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5874

🔧 Running config 1193/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3123

🔧 Running config 1194/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3626

🔧 Running config 1195/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3550

🔧 Running config 1196/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3612

🔧 Running config 1197/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3561

🔧 Running config 1198/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3101

🔧 Running config 1199/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1276

🔧 Running config 1200/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2246

🔧 Running config 1201/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8926

🔧 Running config 1202/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8858

🔧 Running config 1203/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5718

🔧 Running config 1204/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8793

🔧 Running config 1205/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4798

🔧 Running config 1206/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8836

🔧 Running config 1207/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8796

🔧 Running config 1208/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5557

🔧 Running config 1209/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8782

🔧 Running config 1210/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5969

🔧 Running config 1211/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8926

🔧 Running config 1212/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8857

🔧 Running config 1213/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8799

🔧 Running config 1214/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8793

🔧 Running config 1215/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8774

🔧 Running config 1216/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8836

🔧 Running config 1217/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8796

🔧 Running config 1218/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8785

🔧 Running config 1219/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 1220/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8795

🔧 Running config 1221/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8926

🔧 Running config 1222/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8858

🔧 Running config 1223/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8820

🔧 Running config 1224/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8793

🔧 Running config 1225/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0482

🔧 Running config 1226/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8836

🔧 Running config 1227/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8796

🔧 Running config 1228/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8802

🔧 Running config 1229/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8782

🔧 Running config 1230/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0017

🔧 Running config 1231/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8926

🔧 Running config 1232/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8857

🔧 Running config 1233/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8789

🔧 Running config 1234/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8793

🔧 Running config 1235/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8774

🔧 Running config 1236/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8836

🔧 Running config 1237/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8796

🔧 Running config 1238/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6013

🔧 Running config 1239/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 1240/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8796

🔧 Running config 1241/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8967

🔧 Running config 1242/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8985

🔧 Running config 1243/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8908

🔧 Running config 1244/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8801

🔧 Running config 1245/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8749

🔧 Running config 1246/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8865

🔧 Running config 1247/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8873

🔧 Running config 1248/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8802

🔧 Running config 1249/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8792

🔧 Running config 1250/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8518

🔧 Running config 1251/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8967

🔧 Running config 1252/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8986

🔧 Running config 1253/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8908

🔧 Running config 1254/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8802

🔧 Running config 1255/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8809

🔧 Running config 1256/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8865

🔧 Running config 1257/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8871

🔧 Running config 1258/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8805

🔧 Running config 1259/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8798

🔧 Running config 1260/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8777

🔧 Running config 1261/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8967

🔧 Running config 1262/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8986

🔧 Running config 1263/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8907

🔧 Running config 1264/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8783

🔧 Running config 1265/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5016

🔧 Running config 1266/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8865

🔧 Running config 1267/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8872

🔧 Running config 1268/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8802

🔧 Running config 1269/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8787

🔧 Running config 1270/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5535

🔧 Running config 1271/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8967

🔧 Running config 1272/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8986

🔧 Running config 1273/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8907

🔧 Running config 1274/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8791

🔧 Running config 1275/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8808

🔧 Running config 1276/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8865

🔧 Running config 1277/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8871

🔧 Running config 1278/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8805

🔧 Running config 1279/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8792

🔧 Running config 1280/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8776

🔧 Running config 1281/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9149

🔧 Running config 1282/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7635

🔧 Running config 1283/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8449

🔧 Running config 1284/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7712

🔧 Running config 1285/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6620

🔧 Running config 1286/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8009

🔧 Running config 1287/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7232

🔧 Running config 1288/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6714

🔧 Running config 1289/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6737

🔧 Running config 1290/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6068

🔧 Running config 1291/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1264

🔧 Running config 1292/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8438

🔧 Running config 1293/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7525

🔧 Running config 1294/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8174

🔧 Running config 1295/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7526

🔧 Running config 1296/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0596

🔧 Running config 1297/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8123

🔧 Running config 1298/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7293

🔧 Running config 1299/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6434

🔧 Running config 1300/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6805

🔧 Running config 1301/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7885

🔧 Running config 1302/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9018

🔧 Running config 1303/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7239

🔧 Running config 1304/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7279

🔧 Running config 1305/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7820

🔧 Running config 1306/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.5560

🔧 Running config 1307/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6954

🔧 Running config 1308/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6840

🔧 Running config 1309/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6582

🔧 Running config 1310/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.5378

🔧 Running config 1311/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8796

🔧 Running config 1312/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7622

🔧 Running config 1313/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8259

🔧 Running config 1314/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8051

🔧 Running config 1315/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7629

🔧 Running config 1316/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6926

🔧 Running config 1317/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7217

🔧 Running config 1318/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6987

🔧 Running config 1319/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6041

🔧 Running config 1320/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6657

🔧 Running config 1321/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8019

🔧 Running config 1322/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4402

🔧 Running config 1323/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1014

🔧 Running config 1324/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3998

🔧 Running config 1325/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1646

🔧 Running config 1326/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4369

🔧 Running config 1327/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3536

🔧 Running config 1328/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2938

🔧 Running config 1329/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2846

🔧 Running config 1330/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0583

🔧 Running config 1331/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9794

🔧 Running config 1332/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6524

🔧 Running config 1333/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3484

🔧 Running config 1334/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1901

🔧 Running config 1335/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3165

🔧 Running config 1336/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5403

🔧 Running config 1337/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4472

🔧 Running config 1338/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3182

🔧 Running config 1339/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2715

🔧 Running config 1340/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0966

🔧 Running config 1341/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3066

🔧 Running config 1342/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2643

🔧 Running config 1343/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4371

🔧 Running config 1344/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2032

🔧 Running config 1345/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3607

🔧 Running config 1346/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0441

🔧 Running config 1347/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1958

🔧 Running config 1348/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0462

🔧 Running config 1349/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0983

🔧 Running config 1350/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1646

🔧 Running config 1351/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4038

🔧 Running config 1352/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3587

🔧 Running config 1353/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4929

🔧 Running config 1354/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2635

🔧 Running config 1355/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9812

🔧 Running config 1356/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4346

🔧 Running config 1357/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1861

🔧 Running config 1358/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2064

🔧 Running config 1359/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1362

🔧 Running config 1360/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1621

🔧 Running config 1361/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 1362/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8857

🔧 Running config 1363/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8796

🔧 Running config 1364/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6997

🔧 Running config 1365/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3442

🔧 Running config 1366/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8840

🔧 Running config 1367/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8808

🔧 Running config 1368/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8799

🔧 Running config 1369/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3661

🔧 Running config 1370/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3928

🔧 Running config 1371/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 1372/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8858

🔧 Running config 1373/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7488

🔧 Running config 1374/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6816

🔧 Running config 1375/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4693

🔧 Running config 1376/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8841

🔧 Running config 1377/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8799

🔧 Running config 1378/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6309

🔧 Running config 1379/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4900

🔧 Running config 1380/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4910

🔧 Running config 1381/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 1382/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6798

🔧 Running config 1383/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8786

🔧 Running config 1384/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5890

🔧 Running config 1385/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2976

🔧 Running config 1386/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8841

🔧 Running config 1387/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5630

🔧 Running config 1388/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8796

🔧 Running config 1389/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5334

🔧 Running config 1390/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2014

🔧 Running config 1391/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8916

🔧 Running config 1392/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8858

🔧 Running config 1393/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7759

🔧 Running config 1394/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6265

🔧 Running config 1395/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5071

🔧 Running config 1396/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8841

🔧 Running config 1397/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8808

🔧 Running config 1398/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7080

🔧 Running config 1399/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4910

🔧 Running config 1400/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3903

🔧 Running config 1401/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9047

🔧 Running config 1402/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9016

🔧 Running config 1403/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8875

🔧 Running config 1404/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8771

🔧 Running config 1405/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8787

🔧 Running config 1406/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8944

🔧 Running config 1407/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8887

🔧 Running config 1408/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8777

🔧 Running config 1409/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8815

🔧 Running config 1410/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7620

🔧 Running config 1411/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9047

🔧 Running config 1412/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9018

🔧 Running config 1413/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8880

🔧 Running config 1414/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8792

🔧 Running config 1415/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8778

🔧 Running config 1416/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8944

🔧 Running config 1417/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8887

🔧 Running config 1418/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8777

🔧 Running config 1419/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8360

🔧 Running config 1420/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8422

🔧 Running config 1421/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9036

🔧 Running config 1422/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9015

🔧 Running config 1423/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.9007

🔧 Running config 1424/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8799

🔧 Running config 1425/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8798

🔧 Running config 1426/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8834

🔧 Running config 1427/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8887

🔧 Running config 1428/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8777

🔧 Running config 1429/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8815

🔧 Running config 1430/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8788

🔧 Running config 1431/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9047

🔧 Running config 1432/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.9018

🔧 Running config 1433/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8995

🔧 Running config 1434/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8792

🔧 Running config 1435/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8804

🔧 Running config 1436/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8834

🔧 Running config 1437/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8887

🔧 Running config 1438/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8777

🔧 Running config 1439/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6925

🔧 Running config 1440/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8784

🔧 Running config 1441/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4341

🔧 Running config 1442/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0356

🔧 Running config 1443/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0791

🔧 Running config 1444/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0898

🔧 Running config 1445/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0170

🔧 Running config 1446/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1191

🔧 Running config 1447/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0154

🔧 Running config 1448/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1200

🔧 Running config 1449/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0615

🔧 Running config 1450/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2187

🔧 Running config 1451/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2773

🔧 Running config 1452/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4350

🔧 Running config 1453/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2614

🔧 Running config 1454/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5014

🔧 Running config 1455/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4270

🔧 Running config 1456/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2339

🔧 Running config 1457/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3960

🔧 Running config 1458/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4088

🔧 Running config 1459/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4496

🔧 Running config 1460/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5230

🔧 Running config 1461/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0375

🔧 Running config 1462/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1450

🔧 Running config 1463/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1941

🔧 Running config 1464/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1655

🔧 Running config 1465/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0546

🔧 Running config 1466/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0096

🔧 Running config 1467/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0706

🔧 Running config 1468/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9654

🔧 Running config 1469/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1153

🔧 Running config 1470/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2009

🔧 Running config 1471/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2380

🔧 Running config 1472/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2873

🔧 Running config 1473/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2658

🔧 Running config 1474/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4852

🔧 Running config 1475/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3256

🔧 Running config 1476/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0997

🔧 Running config 1477/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2383

🔧 Running config 1478/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2405

🔧 Running config 1479/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5497

🔧 Running config 1480/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.5551

🔧 Running config 1481/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6866

🔧 Running config 1482/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7914

🔧 Running config 1483/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3928

🔧 Running config 1484/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3690

🔧 Running config 1485/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1903

🔧 Running config 1486/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6158

🔧 Running config 1487/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5623

🔧 Running config 1488/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3737

🔧 Running config 1489/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3370

🔧 Running config 1490/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3282

🔧 Running config 1491/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9677

🔧 Running config 1492/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7778

🔧 Running config 1493/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5491

🔧 Running config 1494/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5131

🔧 Running config 1495/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3544

🔧 Running config 1496/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7624

🔧 Running config 1497/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.7499

🔧 Running config 1498/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4419

🔧 Running config 1499/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4861

🔧 Running config 1500/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3805

🔧 Running config 1501/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3036

🔧 Running config 1502/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4316

🔧 Running config 1503/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2581

🔧 Running config 1504/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4577

🔧 Running config 1505/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4993

🔧 Running config 1506/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4093

🔧 Running config 1507/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3972

🔧 Running config 1508/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2883

🔧 Running config 1509/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2423

🔧 Running config 1510/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2775

🔧 Running config 1511/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5200

🔧 Running config 1512/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6176

🔧 Running config 1513/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6037

🔧 Running config 1514/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.5910

🔧 Running config 1515/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.4725

🔧 Running config 1516/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4562

🔧 Running config 1517/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5036

🔧 Running config 1518/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5247

🔧 Running config 1519/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4927

🔧 Running config 1520/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3250

🔧 Running config 1521/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8962

🔧 Running config 1522/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6772

🔧 Running config 1523/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4575

🔧 Running config 1524/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2662

🔧 Running config 1525/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3228

🔧 Running config 1526/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.6178

🔧 Running config 1527/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6198

🔧 Running config 1528/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.2243

🔧 Running config 1529/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2150

🔧 Running config 1530/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.2633

🔧 Running config 1531/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8854

🔧 Running config 1532/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8901

🔧 Running config 1533/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7215

🔧 Running config 1534/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8213

🔧 Running config 1535/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8683

🔧 Running config 1536/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8828

🔧 Running config 1537/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6322

🔧 Running config 1538/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6979

🔧 Running config 1539/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8498

🔧 Running config 1540/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5221

🔧 Running config 1541/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8854

🔧 Running config 1542/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6633

🔧 Running config 1543/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4042

🔧 Running config 1544/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1930

🔧 Running config 1545/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0656

🔧 Running config 1546/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8828

🔧 Running config 1547/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4242

🔧 Running config 1548/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1694

🔧 Running config 1549/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0913

🔧 Running config 1550/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1511

🔧 Running config 1551/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8854

🔧 Running config 1552/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7748

🔧 Running config 1553/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7297

🔧 Running config 1554/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8790

🔧 Running config 1555/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5569

🔧 Running config 1556/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8828

🔧 Running config 1557/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6974

🔧 Running config 1558/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6679

🔧 Running config 1559/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4904

🔧 Running config 1560/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1134

🔧 Running config 1561/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8963

🔧 Running config 1562/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8902

🔧 Running config 1563/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8916

🔧 Running config 1564/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6853

🔧 Running config 1565/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7313

🔧 Running config 1566/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8830

🔧 Running config 1567/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8868

🔧 Running config 1568/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8785

🔧 Running config 1569/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7905

🔧 Running config 1570/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5312

🔧 Running config 1571/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8963

🔧 Running config 1572/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8902

🔧 Running config 1573/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8919

🔧 Running config 1574/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8788

🔧 Running config 1575/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8788

🔧 Running config 1576/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8830

🔧 Running config 1577/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8869

🔧 Running config 1578/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8785

🔧 Running config 1579/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8784

🔧 Running config 1580/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8793

🔧 Running config 1581/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8963

🔧 Running config 1582/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8902

🔧 Running config 1583/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8916

🔧 Running config 1584/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5359

🔧 Running config 1585/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8822

🔧 Running config 1586/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8830

🔧 Running config 1587/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8725

🔧 Running config 1588/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8785

🔧 Running config 1589/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.4705

🔧 Running config 1590/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5958

🔧 Running config 1591/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8963

🔧 Running config 1592/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8902

🔧 Running config 1593/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8918

🔧 Running config 1594/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8794

🔧 Running config 1595/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8812

🔧 Running config 1596/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8830

🔧 Running config 1597/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8869

🔧 Running config 1598/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8785

🔧 Running config 1599/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8784

🔧 Running config 1600/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8202

🔧 Running config 1601/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0598

🔧 Running config 1602/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9269

🔧 Running config 1603/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0240

🔧 Running config 1604/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8931

🔧 Running config 1605/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7927

🔧 Running config 1606/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9772

🔧 Running config 1607/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8335

🔧 Running config 1608/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8045

🔧 Running config 1609/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7523

🔧 Running config 1610/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7121

🔧 Running config 1611/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1553

🔧 Running config 1612/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0457

🔧 Running config 1613/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0246

🔧 Running config 1614/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0212

🔧 Running config 1615/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0788

🔧 Running config 1616/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0923

🔧 Running config 1617/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9652

🔧 Running config 1618/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8769

🔧 Running config 1619/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8638

🔧 Running config 1620/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9715

🔧 Running config 1621/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9400

🔧 Running config 1622/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8907

🔧 Running config 1623/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.9217

🔧 Running config 1624/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9182

🔧 Running config 1625/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8190

🔧 Running config 1626/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7869

🔧 Running config 1627/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7859

🔧 Running config 1628/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7977

🔧 Running config 1629/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7511

🔧 Running config 1630/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8348

🔧 Running config 1631/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9912

🔧 Running config 1632/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0476

🔧 Running config 1633/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.0542

🔧 Running config 1634/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9816

🔧 Running config 1635/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0100

🔧 Running config 1636/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9736

🔧 Running config 1637/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8970

🔧 Running config 1638/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8892

🔧 Running config 1639/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8400

🔧 Running config 1640/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.9603

🔧 Running config 1641/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6675

🔧 Running config 1642/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4012

🔧 Running config 1643/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2907

🔧 Running config 1644/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1290

🔧 Running config 1645/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3997

🔧 Running config 1646/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5242

🔧 Running config 1647/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4096

🔧 Running config 1648/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2448

🔧 Running config 1649/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2665

🔧 Running config 1650/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0575

🔧 Running config 1651/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.8740

🔧 Running config 1652/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.5692

🔧 Running config 1653/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.4393

🔧 Running config 1654/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2463

🔧 Running config 1655/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2222

🔧 Running config 1656/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5929

🔧 Running config 1657/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3132

🔧 Running config 1658/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2514

🔧 Running config 1659/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1139

🔧 Running config 1660/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1349

🔧 Running config 1661/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2043

🔧 Running config 1662/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1566

🔧 Running config 1663/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3879

🔧 Running config 1664/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1806

🔧 Running config 1665/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2332

🔧 Running config 1666/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0457

🔧 Running config 1667/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2007

🔧 Running config 1668/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2174

🔧 Running config 1669/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.0966

🔧 Running config 1670/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1821

🔧 Running config 1671/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6370

🔧 Running config 1672/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3294

🔧 Running config 1673/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3703

🔧 Running config 1674/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1533

🔧 Running config 1675/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3087

🔧 Running config 1676/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.3050

🔧 Running config 1677/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2756

🔧 Running config 1678/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2967

🔧 Running config 1679/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2762

🔧 Running config 1680/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1085

🔧 Running config 1681/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8847

🔧 Running config 1682/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8865

🔧 Running config 1683/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5905

🔧 Running config 1684/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1880

🔧 Running config 1685/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3042

🔧 Running config 1686/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8799

🔧 Running config 1687/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.4486

🔧 Running config 1688/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.1302

🔧 Running config 1689/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8589

🔧 Running config 1690/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0691

🔧 Running config 1691/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8847

🔧 Running config 1692/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8865

🔧 Running config 1693/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7017

🔧 Running config 1694/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5051

🔧 Running config 1695/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4300

🔧 Running config 1696/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8799

🔧 Running config 1697/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8806

🔧 Running config 1698/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6742

🔧 Running config 1699/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3033

🔧 Running config 1700/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.3230

🔧 Running config 1701/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8847

🔧 Running config 1702/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8874

🔧 Running config 1703/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5352

🔧 Running config 1704/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.9523

🔧 Running config 1705/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0470

🔧 Running config 1706/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8799

🔧 Running config 1707/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6162

🔧 Running config 1708/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4204

🔧 Running config 1709/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.8524

🔧 Running config 1710/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1974

🔧 Running config 1711/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8847

🔧 Running config 1712/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8865

🔧 Running config 1713/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7004

🔧 Running config 1714/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3718

🔧 Running config 1715/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8445

🔧 Running config 1716/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8799

🔧 Running config 1717/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8806

🔧 Running config 1718/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7151

🔧 Running config 1719/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.2161

🔧 Running config 1720/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5433

🔧 Running config 1721/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9197

🔧 Running config 1722/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8916

🔧 Running config 1723/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8962

🔧 Running config 1724/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6886

🔧 Running config 1725/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.9191

🔧 Running config 1726/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8815

🔧 Running config 1727/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8790

🔧 Running config 1728/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8820

🔧 Running config 1729/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6650

🔧 Running config 1730/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8779

🔧 Running config 1731/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9198

🔧 Running config 1732/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8917

🔧 Running config 1733/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8907

🔧 Running config 1734/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8752

🔧 Running config 1735/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8444

🔧 Running config 1736/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8815

🔧 Running config 1737/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8790

🔧 Running config 1738/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8821

🔧 Running config 1739/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8436

🔧 Running config 1740/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6851

🔧 Running config 1741/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9198

🔧 Running config 1742/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8916

🔧 Running config 1743/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7727

🔧 Running config 1744/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8359

🔧 Running config 1745/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7073

🔧 Running config 1746/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8815

🔧 Running config 1747/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.6412

🔧 Running config 1748/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6709

🔧 Running config 1749/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8813

🔧 Running config 1750/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6300

🔧 Running config 1751/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.9198

🔧 Running config 1752/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8917

🔧 Running config 1753/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8907

🔧 Running config 1754/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8820

🔧 Running config 1755/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8796

🔧 Running config 1756/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8815

🔧 Running config 1757/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8790

🔧 Running config 1758/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8821

🔧 Running config 1759/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.7181

🔧 Running config 1760/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8782

🔧 Running config 1761/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8577

🔧 Running config 1762/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.9710

🔧 Running config 1763/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8215

🔧 Running config 1764/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7992

🔧 Running config 1765/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7552

🔧 Running config 1766/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8906

🔧 Running config 1767/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7012

🔧 Running config 1768/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6670

🔧 Running config 1769/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6413

🔧 Running config 1770/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.5200

🔧 Running config 1771/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1915

🔧 Running config 1772/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.0005

🔧 Running config 1773/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8687

🔧 Running config 1774/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.9066

🔧 Running config 1775/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7924

🔧 Running config 1776/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.9506

🔧 Running config 1777/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8326

🔧 Running config 1778/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.7291

🔧 Running config 1779/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7675

🔧 Running config 1780/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7081

🔧 Running config 1781/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.8263

🔧 Running config 1782/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7787

🔧 Running config 1783/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8145

🔧 Running config 1784/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7697

🔧 Running config 1785/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.7797

🔧 Running config 1786/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.6917

🔧 Running config 1787/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.6492

🔧 Running config 1788/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.6725

🔧 Running config 1789/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.6652

🔧 Running config 1790/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6533

🔧 Running config 1791/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7326

🔧 Running config 1792/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.8603

🔧 Running config 1793/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8611

🔧 Running config 1794/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.8837

🔧 Running config 1795/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.8424

🔧 Running config 1796/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 1.7823

🔧 Running config 1797/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 1.7069

🔧 Running config 1798/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 1.8292

🔧 Running config 1799/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 1.7719

🔧 Running config 1800/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 1.6898

🔧 Running config 1801/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.7456

🔧 Running config 1802/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4170

🔧 Running config 1803/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.5015

🔧 Running config 1804/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4208

🔧 Running config 1805/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.3122

🔧 Running config 1806/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.5662

🔧 Running config 1807/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3897

🔧 Running config 1808/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3722

🔧 Running config 1809/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.4135

🔧 Running config 1810/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0936

🔧 Running config 1811/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.9305

🔧 Running config 1812/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.6473

🔧 Running config 1813/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.6173

🔧 Running config 1814/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2906

🔧 Running config 1815/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1403

🔧 Running config 1816/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.6769

🔧 Running config 1817/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.4251

🔧 Running config 1818/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.2939

🔧 Running config 1819/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3360

🔧 Running config 1820/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0544

🔧 Running config 1821/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.1863

🔧 Running config 1822/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.1706

🔧 Running config 1823/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1377

🔧 Running config 1824/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.3281

🔧 Running config 1825/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.2203

🔧 Running config 1826/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.0285

🔧 Running config 1827/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3013

🔧 Running config 1828/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1863

🔧 Running config 1829/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2144

🔧 Running config 1830/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.0609

🔧 Running config 1831/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.4157

🔧 Running config 1832/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.3725

🔧 Running config 1833/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.3710

🔧 Running config 1834/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.2065

🔧 Running config 1835/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1245

🔧 Running config 1836/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 2.2893

🔧 Running config 1837/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 2.2027

🔧 Running config 1838/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 2.1152

🔧 Running config 1839/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 2.1135

🔧 Running config 1840/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.1588

🔧 Running config 1841/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8928

🔧 Running config 1842/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8843

🔧 Running config 1843/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6601

🔧 Running config 1844/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3117

🔧 Running config 1845/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.0923

🔧 Running config 1846/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8793

🔧 Running config 1847/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8782

🔧 Running config 1848/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.4379

🔧 Running config 1849/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0705

🔧 Running config 1850/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9932

🔧 Running config 1851/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8833

🔧 Running config 1852/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8815

🔧 Running config 1853/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7280

🔧 Running config 1854/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8376

🔧 Running config 1855/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6472

🔧 Running config 1856/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8793

🔧 Running config 1857/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8782

🔧 Running config 1858/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7265

🔧 Running config 1859/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8317

🔧 Running config 1860/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4506

🔧 Running config 1861/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7480

🔧 Running config 1862/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.7286

🔧 Running config 1863/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5142

🔧 Running config 1864/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.1364

🔧 Running config 1865/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.1163

🔧 Running config 1866/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.7312

🔧 Running config 1867/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.5321

🔧 Running config 1868/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.5095

🔧 Running config 1869/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.0500

🔧 Running config 1870/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 2.9421

🔧 Running config 1871/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8928

🔧 Running config 1872/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8849

🔧 Running config 1873/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7408

🔧 Running config 1874/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.6638

🔧 Running config 1875/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5411

🔧 Running config 1876/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8793

🔧 Running config 1877/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8810

🔧 Running config 1878/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6358

🔧 Running config 1879/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5820

🔧 Running config 1880/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4224

🔧 Running config 1881/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8924

🔧 Running config 1882/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8936

🔧 Running config 1883/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8876

🔧 Running config 1884/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8823

🔧 Running config 1885/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8774

🔧 Running config 1886/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8809

🔧 Running config 1887/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8806

🔧 Running config 1888/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8868

🔧 Running config 1889/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8798

🔧 Running config 1890/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.8729

🔧 Running config 1891/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8924

🔧 Running config 1892/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8936

🔧 Running config 1893/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7940

🔧 Running config 1894/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8791

🔧 Running config 1895/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.7013

🔧 Running config 1896/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8809

🔧 Running config 1897/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8807

🔧 Running config 1898/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7611

🔧 Running config 1899/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8802

🔧 Running config 1900/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6909

🔧 Running config 1901/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8924

🔧 Running config 1902/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8900

🔧 Running config 1903/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.7625

🔧 Running config 1904/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.5537

🔧 Running config 1905/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6146

🔧 Running config 1906/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8809

🔧 Running config 1907/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8806

🔧 Running config 1908/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.6341

🔧 Running config 1909/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.3228

🔧 Running config 1910/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.5922

🔧 Running config 1911/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8925

🔧 Running config 1912/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8936

🔧 Running config 1913/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8908

🔧 Running config 1914/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8794

🔧 Running config 1915/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.6430

🔧 Running config 1916/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
✅ Final Val Loss: 3.8809

🔧 Running config 1917/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
✅ Final Val Loss: 3.8807

🔧 Running config 1918/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
✅ Final Val Loss: 3.8793

🔧 Running config 1919/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
✅ Final Val Loss: 3.8789

🔧 Running config 1920/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
✅ Final Val Loss: 3.4909

🏆 Best config: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
📉 Best validation loss: 1.5128
