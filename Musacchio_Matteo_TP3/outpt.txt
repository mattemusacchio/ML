ğŸ” Probing 1920 configurations...

ğŸ”§ Running config 1/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9824

ğŸ”§ Running config 2/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9447

ğŸ”§ Running config 3/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9108

ğŸ”§ Running config 4/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9802

ğŸ”§ Running config 5/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9376

ğŸ”§ Running config 6/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8726

ğŸ”§ Running config 7/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9611

ğŸ”§ Running config 8/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9471

ğŸ”§ Running config 9/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0147

ğŸ”§ Running config 10/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1546

ğŸ”§ Running config 11/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1106

ğŸ”§ Running config 12/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0574

ğŸ”§ Running config 13/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1135

ğŸ”§ Running config 14/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1452

ğŸ”§ Running config 15/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1067

ğŸ”§ Running config 16/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9987

ğŸ”§ Running config 17/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0957

ğŸ”§ Running config 18/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1255

ğŸ”§ Running config 19/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2131

ğŸ”§ Running config 20/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3526

ğŸ”§ Running config 21/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9105

ğŸ”§ Running config 22/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9206

ğŸ”§ Running config 23/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8797

ğŸ”§ Running config 24/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9748

ğŸ”§ Running config 25/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9358

ğŸ”§ Running config 26/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8890

ğŸ”§ Running config 27/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9456

ğŸ”§ Running config 28/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9421

ğŸ”§ Running config 29/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0125

ğŸ”§ Running config 30/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1535

ğŸ”§ Running config 31/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0195

ğŸ”§ Running config 32/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0068

ğŸ”§ Running config 33/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0990

ğŸ”§ Running config 34/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1396

ğŸ”§ Running config 35/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0961

ğŸ”§ Running config 36/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9411

ğŸ”§ Running config 37/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0658

ğŸ”§ Running config 38/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1170

ğŸ”§ Running config 39/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2085

ğŸ”§ Running config 40/1920: {'hidden_layers': [32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3522

ğŸ”§ Running config 41/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9217

ğŸ”§ Running config 42/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5384

ğŸ”§ Running config 43/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7078

ğŸ”§ Running config 44/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2958

ğŸ”§ Running config 45/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3391

ğŸ”§ Running config 46/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8372

ğŸ”§ Running config 47/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4456

ğŸ”§ Running config 48/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3070

ğŸ”§ Running config 49/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2019

ğŸ”§ Running config 50/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2280

ğŸ”§ Running config 51/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.0924

ğŸ”§ Running config 52/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7475

ğŸ”§ Running config 53/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6645

ğŸ”§ Running config 54/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4874

ğŸ”§ Running config 55/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5862

ğŸ”§ Running config 56/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9168

ğŸ”§ Running config 57/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5746

ğŸ”§ Running config 58/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5399

ğŸ”§ Running config 59/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4237

ğŸ”§ Running config 60/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4488

ğŸ”§ Running config 61/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5755

ğŸ”§ Running config 62/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6113

ğŸ”§ Running config 63/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4460

ğŸ”§ Running config 64/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3543

ğŸ”§ Running config 65/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2754

ğŸ”§ Running config 66/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4790

ğŸ”§ Running config 67/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4149

ğŸ”§ Running config 68/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2774

ğŸ”§ Running config 69/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3173

ğŸ”§ Running config 70/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2226

ğŸ”§ Running config 71/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7693

ğŸ”§ Running config 72/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4439

ğŸ”§ Running config 73/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5581

ğŸ”§ Running config 74/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4611

ğŸ”§ Running config 75/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5246

ğŸ”§ Running config 76/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7734

ğŸ”§ Running config 77/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4955

ğŸ”§ Running config 78/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4570

ğŸ”§ Running config 79/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3233

ğŸ”§ Running config 80/1920: {'hidden_layers': [32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3873

ğŸ”§ Running config 81/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7285

ğŸ”§ Running config 82/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7188

ğŸ”§ Running config 83/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3126

ğŸ”§ Running config 84/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2430

ğŸ”§ Running config 85/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3625

ğŸ”§ Running config 86/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.4796

ğŸ”§ Running config 87/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5828

ğŸ”§ Running config 88/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2648

ğŸ”§ Running config 89/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2211

ğŸ”§ Running config 90/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3092

ğŸ”§ Running config 91/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 92/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8077

ğŸ”§ Running config 93/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5039

ğŸ”§ Running config 94/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3561

ğŸ”§ Running config 95/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2516

ğŸ”§ Running config 96/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8812

ğŸ”§ Running config 97/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4923

ğŸ”§ Running config 98/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3188

ğŸ”§ Running config 99/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2449

ğŸ”§ Running config 100/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0953

ğŸ”§ Running config 101/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8947

ğŸ”§ Running config 102/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4430

ğŸ”§ Running config 103/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4228

ğŸ”§ Running config 104/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0225

ğŸ”§ Running config 105/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2684

ğŸ”§ Running config 106/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.6377

ğŸ”§ Running config 107/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1319

ğŸ”§ Running config 108/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9933

ğŸ”§ Running config 109/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9538

ğŸ”§ Running config 110/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1652

ğŸ”§ Running config 111/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 112/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5198

ğŸ”§ Running config 113/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2468

ğŸ”§ Running config 114/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2454

ğŸ”§ Running config 115/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1615

ğŸ”§ Running config 116/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8812

ğŸ”§ Running config 117/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2293

ğŸ”§ Running config 118/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9298

ğŸ”§ Running config 119/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0395

ğŸ”§ Running config 120/1920: {'hidden_layers': [32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2314

ğŸ”§ Running config 121/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9156

ğŸ”§ Running config 122/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8303

ğŸ”§ Running config 123/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8133

ğŸ”§ Running config 124/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7554

ğŸ”§ Running config 125/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6227

ğŸ”§ Running config 126/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 127/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7738

ğŸ”§ Running config 128/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7651

ğŸ”§ Running config 129/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7438

ğŸ”§ Running config 130/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4389

ğŸ”§ Running config 131/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9157

ğŸ”§ Running config 132/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8492

ğŸ”§ Running config 133/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7944

ğŸ”§ Running config 134/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7964

ğŸ”§ Running config 135/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6960

ğŸ”§ Running config 136/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 137/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8440

ğŸ”§ Running config 138/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7890

ğŸ”§ Running config 139/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7672

ğŸ”§ Running config 140/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6663

ğŸ”§ Running config 141/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9156

ğŸ”§ Running config 142/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8928

ğŸ”§ Running config 143/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8298

ğŸ”§ Running config 144/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7531

ğŸ”§ Running config 145/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7119

ğŸ”§ Running config 146/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 147/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6097

ğŸ”§ Running config 148/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8179

ğŸ”§ Running config 149/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4119

ğŸ”§ Running config 150/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5345

ğŸ”§ Running config 151/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9157

ğŸ”§ Running config 152/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8501

ğŸ”§ Running config 153/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8570

ğŸ”§ Running config 154/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7732

ğŸ”§ Running config 155/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6559

ğŸ”§ Running config 156/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 157/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8904

ğŸ”§ Running config 158/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7780

ğŸ”§ Running config 159/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7342

ğŸ”§ Running config 160/1920: {'hidden_layers': [32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6252

ğŸ”§ Running config 161/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8795

ğŸ”§ Running config 162/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7372

ğŸ”§ Running config 163/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7896

ğŸ”§ Running config 164/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6992

ğŸ”§ Running config 165/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7007

ğŸ”§ Running config 166/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7008

ğŸ”§ Running config 167/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6746

ğŸ”§ Running config 168/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6831

ğŸ”§ Running config 169/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7327

ğŸ”§ Running config 170/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8232

ğŸ”§ Running config 171/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8371

ğŸ”§ Running config 172/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8646

ğŸ”§ Running config 173/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9113

ğŸ”§ Running config 174/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8521

ğŸ”§ Running config 175/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8321

ğŸ”§ Running config 176/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8094

ğŸ”§ Running config 177/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8032

ğŸ”§ Running config 178/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8106

ğŸ”§ Running config 179/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8587

ğŸ”§ Running config 180/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9747

ğŸ”§ Running config 181/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7422

ğŸ”§ Running config 182/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7044

ğŸ”§ Running config 183/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7688

ğŸ”§ Running config 184/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6965

ğŸ”§ Running config 185/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6976

ğŸ”§ Running config 186/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6584

ğŸ”§ Running config 187/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6617

ğŸ”§ Running config 188/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7080

ğŸ”§ Running config 189/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7348

ğŸ”§ Running config 190/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8232

ğŸ”§ Running config 191/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7830

ğŸ”§ Running config 192/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8946

ğŸ”§ Running config 193/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8848

ğŸ”§ Running config 194/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8446

ğŸ”§ Running config 195/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8295

ğŸ”§ Running config 196/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7418

ğŸ”§ Running config 197/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7850

ğŸ”§ Running config 198/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8102

ğŸ”§ Running config 199/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8607

ğŸ”§ Running config 200/1920: {'hidden_layers': [64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9736

ğŸ”§ Running config 201/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5039

ğŸ”§ Running config 202/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6672

ğŸ”§ Running config 203/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2140

ğŸ”§ Running config 204/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2517

ğŸ”§ Running config 205/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0031

ğŸ”§ Running config 206/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5340

ğŸ”§ Running config 207/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3426

ğŸ”§ Running config 208/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0853

ğŸ”§ Running config 209/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0352

ğŸ”§ Running config 210/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8464

ğŸ”§ Running config 211/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6972

ğŸ”§ Running config 212/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5809

ğŸ”§ Running config 213/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5335

ğŸ”§ Running config 214/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1963

ğŸ”§ Running config 215/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0584

ğŸ”§ Running config 216/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7108

ğŸ”§ Running config 217/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4245

ğŸ”§ Running config 218/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2849

ğŸ”§ Running config 219/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0820

ğŸ”§ Running config 220/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0325

ğŸ”§ Running config 221/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4101

ğŸ”§ Running config 222/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2826

ğŸ”§ Running config 223/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2806

ğŸ”§ Running config 224/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2318

ğŸ”§ Running config 225/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9749

ğŸ”§ Running config 226/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1397

ğŸ”§ Running config 227/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0777

ğŸ”§ Running config 228/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0127

ğŸ”§ Running config 229/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9704

ğŸ”§ Running config 230/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8028

ğŸ”§ Running config 231/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4854

ğŸ”§ Running config 232/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4332

ğŸ”§ Running config 233/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5192

ğŸ”§ Running config 234/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2493

ğŸ”§ Running config 235/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0772

ğŸ”§ Running config 236/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1689

ğŸ”§ Running config 237/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2401

ğŸ”§ Running config 238/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3067

ğŸ”§ Running config 239/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0352

ğŸ”§ Running config 240/1920: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0335

ğŸ”§ Running config 241/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8001

ğŸ”§ Running config 242/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6229

ğŸ”§ Running config 243/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1472

ğŸ”§ Running config 244/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1053

ğŸ”§ Running config 245/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3091

ğŸ”§ Running config 246/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.6406

ğŸ”§ Running config 247/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1577

ğŸ”§ Running config 248/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9820

ğŸ”§ Running config 249/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0448

ğŸ”§ Running config 250/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9573

ğŸ”§ Running config 251/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7616

ğŸ”§ Running config 252/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6488

ğŸ”§ Running config 253/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2710

ğŸ”§ Running config 254/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2362

ğŸ”§ Running config 255/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1483

ğŸ”§ Running config 256/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7289

ğŸ”§ Running config 257/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2970

ğŸ”§ Running config 258/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2247

ğŸ”§ Running config 259/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0848

ğŸ”§ Running config 260/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0392

ğŸ”§ Running config 261/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8913

ğŸ”§ Running config 262/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.3225

ğŸ”§ Running config 263/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9427

ğŸ”§ Running config 264/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8965

ğŸ”§ Running config 265/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2519

ğŸ”§ Running config 266/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.2070

ğŸ”§ Running config 267/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.8609

ğŸ”§ Running config 268/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.8808

ğŸ”§ Running config 269/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.7210

ğŸ”§ Running config 270/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.8126

ğŸ”§ Running config 271/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8958

ğŸ”§ Running config 272/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6687

ğŸ”§ Running config 273/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1752

ğŸ”§ Running config 274/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9479

ğŸ”§ Running config 275/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1694

ğŸ”§ Running config 276/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.3488

ğŸ”§ Running config 277/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1795

ğŸ”§ Running config 278/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7348

ğŸ”§ Running config 279/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8705

ğŸ”§ Running config 280/1920: {'hidden_layers': [64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0060

ğŸ”§ Running config 281/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8971

ğŸ”§ Running config 282/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8912

ğŸ”§ Running config 283/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8080

ğŸ”§ Running config 284/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7229

ğŸ”§ Running config 285/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6285

ğŸ”§ Running config 286/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 287/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7570

ğŸ”§ Running config 288/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6500

ğŸ”§ Running config 289/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5214

ğŸ”§ Running config 290/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3792

ğŸ”§ Running config 291/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8971

ğŸ”§ Running config 292/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8153

ğŸ”§ Running config 293/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7187

ğŸ”§ Running config 294/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6314

ğŸ”§ Running config 295/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7022

ğŸ”§ Running config 296/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 297/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7897

ğŸ”§ Running config 298/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7201

ğŸ”§ Running config 299/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5891

ğŸ”§ Running config 300/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6025

ğŸ”§ Running config 301/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8971

ğŸ”§ Running config 302/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9080

ğŸ”§ Running config 303/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7507

ğŸ”§ Running config 304/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7803

ğŸ”§ Running config 305/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4218

ğŸ”§ Running config 306/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 307/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8339

ğŸ”§ Running config 308/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2325

ğŸ”§ Running config 309/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3100

ğŸ”§ Running config 310/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1984

ğŸ”§ Running config 311/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8896

ğŸ”§ Running config 312/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9081

ğŸ”§ Running config 313/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7986

ğŸ”§ Running config 314/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6203

ğŸ”§ Running config 315/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6930

ğŸ”§ Running config 316/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8878

ğŸ”§ Running config 317/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6617

ğŸ”§ Running config 318/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8045

ğŸ”§ Running config 319/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4238

ğŸ”§ Running config 320/1920: {'hidden_layers': [64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3589

ğŸ”§ Running config 321/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6810

ğŸ”§ Running config 322/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6872

ğŸ”§ Running config 323/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6664

ğŸ”§ Running config 324/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6386

ğŸ”§ Running config 325/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6216

ğŸ”§ Running config 326/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5251

ğŸ”§ Running config 327/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.5764

ğŸ”§ Running config 328/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.5807

ğŸ”§ Running config 329/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.5884

ğŸ”§ Running config 330/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6345

ğŸ”§ Running config 331/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6631

ğŸ”§ Running config 332/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6647

ğŸ”§ Running config 333/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6242

ğŸ”§ Running config 334/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6477

ğŸ”§ Running config 335/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6553

ğŸ”§ Running config 336/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6570

ğŸ”§ Running config 337/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6418

ğŸ”§ Running config 338/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6314

ğŸ”§ Running config 339/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6401

ğŸ”§ Running config 340/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7028

ğŸ”§ Running config 341/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5906

ğŸ”§ Running config 342/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6130

ğŸ”§ Running config 343/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6410

ğŸ”§ Running config 344/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6259

ğŸ”§ Running config 345/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6127

ğŸ”§ Running config 346/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5128

ğŸ”§ Running config 347/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.5514

ğŸ”§ Running config 348/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.5625

ğŸ”§ Running config 349/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.5896

ğŸ”§ Running config 350/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6352

ğŸ”§ Running config 351/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6786

ğŸ”§ Running config 352/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6943

ğŸ”§ Running config 353/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7289

ğŸ”§ Running config 354/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6461

ğŸ”§ Running config 355/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6599

ğŸ”§ Running config 356/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5750

ğŸ”§ Running config 357/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6132

ğŸ”§ Running config 358/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6311

ğŸ”§ Running config 359/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6415

ğŸ”§ Running config 360/1920: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7018

ğŸ”§ Running config 361/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9549

ğŸ”§ Running config 362/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7681

ğŸ”§ Running config 363/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1800

ğŸ”§ Running config 364/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9656

ğŸ”§ Running config 365/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8688

ğŸ”§ Running config 366/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7181

ğŸ”§ Running config 367/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2085

ğŸ”§ Running config 368/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9679

ğŸ”§ Running config 369/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7556

ğŸ”§ Running config 370/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6820

ğŸ”§ Running config 371/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8784

ğŸ”§ Running config 372/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6339

ğŸ”§ Running config 373/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4143

ğŸ”§ Running config 374/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1933

ğŸ”§ Running config 375/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0822

ğŸ”§ Running config 376/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4587

ğŸ”§ Running config 377/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2348

ğŸ”§ Running config 378/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0507

ğŸ”§ Running config 379/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9948

ğŸ”§ Running config 380/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8169

ğŸ”§ Running config 381/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2451

ğŸ”§ Running config 382/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2689

ğŸ”§ Running config 383/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0377

ğŸ”§ Running config 384/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8984

ğŸ”§ Running config 385/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8321

ğŸ”§ Running config 386/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7312

ğŸ”§ Running config 387/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9301

ğŸ”§ Running config 388/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7676

ğŸ”§ Running config 389/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8210

ğŸ”§ Running config 390/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6719

ğŸ”§ Running config 391/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1226

ğŸ”§ Running config 392/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2946

ğŸ”§ Running config 393/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1285

ğŸ”§ Running config 394/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1208

ğŸ”§ Running config 395/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8375

ğŸ”§ Running config 396/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9987

ğŸ”§ Running config 397/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0308

ğŸ”§ Running config 398/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9327

ğŸ”§ Running config 399/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9550

ğŸ”§ Running config 400/1920: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7761

ğŸ”§ Running config 401/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7509

ğŸ”§ Running config 402/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5686

ğŸ”§ Running config 403/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1198

ğŸ”§ Running config 404/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8338

ğŸ”§ Running config 405/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9399

ğŸ”§ Running config 406/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.2610

ğŸ”§ Running config 407/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2121

ğŸ”§ Running config 408/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9903

ğŸ”§ Running config 409/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0260

ğŸ”§ Running config 410/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.6759

ğŸ”§ Running config 411/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7452

ğŸ”§ Running config 412/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5685

ğŸ”§ Running config 413/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5326

ğŸ”§ Running config 414/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1871

ğŸ”§ Running config 415/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9989

ğŸ”§ Running config 416/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.3483

ğŸ”§ Running config 417/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2734

ğŸ”§ Running config 418/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9644

ğŸ”§ Running config 419/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9996

ğŸ”§ Running config 420/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0732

ğŸ”§ Running config 421/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7967

ğŸ”§ Running config 422/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6170

ğŸ”§ Running config 423/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.0001

ğŸ”§ Running config 424/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.7680

ğŸ”§ Running config 425/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.6419

ğŸ”§ Running config 426/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.3651

ğŸ”§ Running config 427/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.8028

ğŸ”§ Running config 428/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6858

ğŸ”§ Running config 429/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5354

ğŸ”§ Running config 430/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.6373

ğŸ”§ Running config 431/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8650

ğŸ”§ Running config 432/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5224

ğŸ”§ Running config 433/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.0337

ğŸ”§ Running config 434/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0556

ğŸ”§ Running config 435/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.7223

ğŸ”§ Running config 436/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.3665

ğŸ”§ Running config 437/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.8796

ğŸ”§ Running config 438/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6389

ğŸ”§ Running config 439/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8239

ğŸ”§ Running config 440/1920: {'hidden_layers': [128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.7551

ğŸ”§ Running config 441/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9209

ğŸ”§ Running config 442/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8319

ğŸ”§ Running config 443/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6792

ğŸ”§ Running config 444/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4284

ğŸ”§ Running config 445/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4019

ğŸ”§ Running config 446/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7758

ğŸ”§ Running config 447/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7264

ğŸ”§ Running config 448/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5398

ğŸ”§ Running config 449/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2120

ğŸ”§ Running config 450/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1736

ğŸ”§ Running config 451/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9086

ğŸ”§ Running config 452/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8358

ğŸ”§ Running config 453/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7236

ğŸ”§ Running config 454/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6419

ğŸ”§ Running config 455/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7098

ğŸ”§ Running config 456/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8945

ğŸ”§ Running config 457/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7510

ğŸ”§ Running config 458/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6114

ğŸ”§ Running config 459/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2822

ğŸ”§ Running config 460/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1477

ğŸ”§ Running config 461/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9210

ğŸ”§ Running config 462/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8536

ğŸ”§ Running config 463/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8039

ğŸ”§ Running config 464/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3953

ğŸ”§ Running config 465/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3362

ğŸ”§ Running config 466/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 467/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8181

ğŸ”§ Running config 468/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.0973

ğŸ”§ Running config 469/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8926

ğŸ”§ Running config 470/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9809

ğŸ”§ Running config 471/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9209

ğŸ”§ Running config 472/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8990

ğŸ”§ Running config 473/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7674

ğŸ”§ Running config 474/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4577

ğŸ”§ Running config 475/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2509

ğŸ”§ Running config 476/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 477/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8850

ğŸ”§ Running config 478/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1450

ğŸ”§ Running config 479/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8638

ğŸ”§ Running config 480/1920: {'hidden_layers': [128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1070

ğŸ”§ Running config 481/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3346

ğŸ”§ Running config 482/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1491

ğŸ”§ Running config 483/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9833

ğŸ”§ Running config 484/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0584

ğŸ”§ Running config 485/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1010

ğŸ”§ Running config 486/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1349

ğŸ”§ Running config 487/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9655

ğŸ”§ Running config 488/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9679

ğŸ”§ Running config 489/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9688

ğŸ”§ Running config 490/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1459

ğŸ”§ Running config 491/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2868

ğŸ”§ Running config 492/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2649

ğŸ”§ Running config 493/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2548

ğŸ”§ Running config 494/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2179

ğŸ”§ Running config 495/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1808

ğŸ”§ Running config 496/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2457

ğŸ”§ Running config 497/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2076

ğŸ”§ Running config 498/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2480

ğŸ”§ Running config 499/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3375

ğŸ”§ Running config 500/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4437

ğŸ”§ Running config 501/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0863

ğŸ”§ Running config 502/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0806

ğŸ”§ Running config 503/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0764

ğŸ”§ Running config 504/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0374

ğŸ”§ Running config 505/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0805

ğŸ”§ Running config 506/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9839

ğŸ”§ Running config 507/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9189

ğŸ”§ Running config 508/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0126

ğŸ”§ Running config 509/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9777

ğŸ”§ Running config 510/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1472

ğŸ”§ Running config 511/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0584

ğŸ”§ Running config 512/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1060

ğŸ”§ Running config 513/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2380

ğŸ”§ Running config 514/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1892

ğŸ”§ Running config 515/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2300

ğŸ”§ Running config 516/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1161

ğŸ”§ Running config 517/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2171

ğŸ”§ Running config 518/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2503

ğŸ”§ Running config 519/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3322

ğŸ”§ Running config 520/1920: {'hidden_layers': [64, 32], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4377

ğŸ”§ Running config 521/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5626

ğŸ”§ Running config 522/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2968

ğŸ”§ Running config 523/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2251

ğŸ”§ Running config 524/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3196

ğŸ”§ Running config 525/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1115

ğŸ”§ Running config 526/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4727

ğŸ”§ Running config 527/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3232

ğŸ”§ Running config 528/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2135

ğŸ”§ Running config 529/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3177

ğŸ”§ Running config 530/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1373

ğŸ”§ Running config 531/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7078

ğŸ”§ Running config 532/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5161

ğŸ”§ Running config 533/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4191

ğŸ”§ Running config 534/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5414

ğŸ”§ Running config 535/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4436

ğŸ”§ Running config 536/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8104

ğŸ”§ Running config 537/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4964

ğŸ”§ Running config 538/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5002

ğŸ”§ Running config 539/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4193

ğŸ”§ Running config 540/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4270

ğŸ”§ Running config 541/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2457

ğŸ”§ Running config 542/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4146

ğŸ”§ Running config 543/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3196

ğŸ”§ Running config 544/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3618

ğŸ”§ Running config 545/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1046

ğŸ”§ Running config 546/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1630

ğŸ”§ Running config 547/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2040

ğŸ”§ Running config 548/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1780

ğŸ”§ Running config 549/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1288

ğŸ”§ Running config 550/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1198

ğŸ”§ Running config 551/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5514

ğŸ”§ Running config 552/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5183

ğŸ”§ Running config 553/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5878

ğŸ”§ Running config 554/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4809

ğŸ”§ Running config 555/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3814

ğŸ”§ Running config 556/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4763

ğŸ”§ Running config 557/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3308

ğŸ”§ Running config 558/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3913

ğŸ”§ Running config 559/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3623

ğŸ”§ Running config 560/1920: {'hidden_layers': [64, 32], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4076

ğŸ”§ Running config 561/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 562/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 563/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8842

ğŸ”§ Running config 564/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8760

ğŸ”§ Running config 565/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 566/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 567/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 568/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 569/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8765

ğŸ”§ Running config 570/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 571/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 572/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 573/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7453

ğŸ”§ Running config 574/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 575/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 576/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 577/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 578/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5612

ğŸ”§ Running config 579/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8803

ğŸ”§ Running config 580/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9746

ğŸ”§ Running config 581/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 582/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 583/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8842

ğŸ”§ Running config 584/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 585/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 586/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 587/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6233

ğŸ”§ Running config 588/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 589/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8765

ğŸ”§ Running config 590/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 591/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8933

ğŸ”§ Running config 592/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 593/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 594/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 595/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3772

ğŸ”§ Running config 596/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 597/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 598/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8778

ğŸ”§ Running config 599/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8803

ğŸ”§ Running config 600/1920: {'hidden_layers': [64, 32], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1096

ğŸ”§ Running config 601/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9114

ğŸ”§ Running config 602/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9009

ğŸ”§ Running config 603/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8823

ğŸ”§ Running config 604/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8767

ğŸ”§ Running config 605/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 606/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 607/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 608/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 609/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 610/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 611/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9114

ğŸ”§ Running config 612/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9009

ğŸ”§ Running config 613/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8824

ğŸ”§ Running config 614/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8774

ğŸ”§ Running config 615/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8804

ğŸ”§ Running config 616/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 617/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 618/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 619/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8803

ğŸ”§ Running config 620/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 621/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9114

ğŸ”§ Running config 622/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9009

ğŸ”§ Running config 623/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8823

ğŸ”§ Running config 624/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8768

ğŸ”§ Running config 625/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 626/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 627/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 628/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 629/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 630/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 631/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9114

ğŸ”§ Running config 632/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9009

ğŸ”§ Running config 633/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8823

ğŸ”§ Running config 634/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8765

ğŸ”§ Running config 635/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8803

ğŸ”§ Running config 636/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 637/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 638/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 639/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8803

ğŸ”§ Running config 640/1920: {'hidden_layers': [64, 32], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 641/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0694

ğŸ”§ Running config 642/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7847

ğŸ”§ Running config 643/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7065

ğŸ”§ Running config 644/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8314

ğŸ”§ Running config 645/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7846

ğŸ”§ Running config 646/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7831

ğŸ”§ Running config 647/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6520

ğŸ”§ Running config 648/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7106

ğŸ”§ Running config 649/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7009

ğŸ”§ Running config 650/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7865

ğŸ”§ Running config 651/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9332

ğŸ”§ Running config 652/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9339

ğŸ”§ Running config 653/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9669

ğŸ”§ Running config 654/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8797

ğŸ”§ Running config 655/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9159

ğŸ”§ Running config 656/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8523

ğŸ”§ Running config 657/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8287

ğŸ”§ Running config 658/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8307

ğŸ”§ Running config 659/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8417

ğŸ”§ Running config 660/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9104

ğŸ”§ Running config 661/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8063

ğŸ”§ Running config 662/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6942

ğŸ”§ Running config 663/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6883

ğŸ”§ Running config 664/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8066

ğŸ”§ Running config 665/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7885

ğŸ”§ Running config 666/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6599

ğŸ”§ Running config 667/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6171

ğŸ”§ Running config 668/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6795

ğŸ”§ Running config 669/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6987

ğŸ”§ Running config 670/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7763

ğŸ”§ Running config 671/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8890

ğŸ”§ Running config 672/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9095

ğŸ”§ Running config 673/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9223

ğŸ”§ Running config 674/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8511

ğŸ”§ Running config 675/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9409

ğŸ”§ Running config 676/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7567

ğŸ”§ Running config 677/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8061

ğŸ”§ Running config 678/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8296

ğŸ”§ Running config 679/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8420

ğŸ”§ Running config 680/1920: {'hidden_layers': [128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9452

ğŸ”§ Running config 681/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4298

ğŸ”§ Running config 682/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7949

ğŸ”§ Running config 683/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3696

ğŸ”§ Running config 684/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3870

ğŸ”§ Running config 685/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1668

ğŸ”§ Running config 686/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4573

ğŸ”§ Running config 687/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4623

ğŸ”§ Running config 688/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2508

ğŸ”§ Running config 689/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1245

ğŸ”§ Running config 690/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9351

ğŸ”§ Running config 691/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7385

ğŸ”§ Running config 692/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5083

ğŸ”§ Running config 693/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3160

ğŸ”§ Running config 694/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3101

ğŸ”§ Running config 695/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1719

ğŸ”§ Running config 696/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5849

ğŸ”§ Running config 697/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2445

ğŸ”§ Running config 698/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0554

ğŸ”§ Running config 699/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3533

ğŸ”§ Running config 700/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0473

ğŸ”§ Running config 701/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2991

ğŸ”§ Running config 702/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2791

ğŸ”§ Running config 703/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2204

ğŸ”§ Running config 704/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2145

ğŸ”§ Running config 705/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0525

ğŸ”§ Running config 706/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1007

ğŸ”§ Running config 707/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1919

ğŸ”§ Running config 708/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2763

ğŸ”§ Running config 709/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0837

ğŸ”§ Running config 710/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8845

ğŸ”§ Running config 711/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2844

ğŸ”§ Running config 712/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3481

ğŸ”§ Running config 713/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3829

ğŸ”§ Running config 714/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4344

ğŸ”§ Running config 715/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1503

ğŸ”§ Running config 716/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1751

ğŸ”§ Running config 717/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4012

ğŸ”§ Running config 718/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2748

ğŸ”§ Running config 719/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1899

ğŸ”§ Running config 720/1920: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9941

ğŸ”§ Running config 721/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9057

ğŸ”§ Running config 722/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4667

ğŸ”§ Running config 723/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7870

ğŸ”§ Running config 724/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0616

ğŸ”§ Running config 725/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9483

ğŸ”§ Running config 726/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 727/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2498

ğŸ”§ Running config 728/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1474

ğŸ”§ Running config 729/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9346

ğŸ”§ Running config 730/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.8477

ğŸ”§ Running config 731/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9057

ğŸ”§ Running config 732/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8822

ğŸ”§ Running config 733/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8500

ğŸ”§ Running config 734/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 735/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9141

ğŸ”§ Running config 736/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 737/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5603

ğŸ”§ Running config 738/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8068

ğŸ”§ Running config 739/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2370

ğŸ”§ Running config 740/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9247

ğŸ”§ Running config 741/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9058

ğŸ”§ Running config 742/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6220

ğŸ”§ Running config 743/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3759

ğŸ”§ Running config 744/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8027

ğŸ”§ Running config 745/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0130

ğŸ”§ Running config 746/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 747/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2715

ğŸ”§ Running config 748/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9224

ğŸ”§ Running config 749/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8033

ğŸ”§ Running config 750/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.8685

ğŸ”§ Running config 751/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9057

ğŸ”§ Running config 752/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8436

ğŸ”§ Running config 753/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8674

ğŸ”§ Running config 754/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4083

ğŸ”§ Running config 755/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9887

ğŸ”§ Running config 756/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 757/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5097

ğŸ”§ Running config 758/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5409

ğŸ”§ Running config 759/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9378

ğŸ”§ Running config 760/1920: {'hidden_layers': [128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9896

ğŸ”§ Running config 761/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9363

ğŸ”§ Running config 762/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9140

ğŸ”§ Running config 763/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8951

ğŸ”§ Running config 764/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 765/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8205

ğŸ”§ Running config 766/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 767/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8877

ğŸ”§ Running config 768/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8831

ğŸ”§ Running config 769/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8825

ğŸ”§ Running config 770/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7473

ğŸ”§ Running config 771/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9365

ğŸ”§ Running config 772/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9142

ğŸ”§ Running config 773/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8952

ğŸ”§ Running config 774/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 775/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8841

ğŸ”§ Running config 776/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 777/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 778/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8804

ğŸ”§ Running config 779/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 780/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8818

ğŸ”§ Running config 781/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9363

ğŸ”§ Running config 782/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9140

ğŸ”§ Running config 783/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6597

ğŸ”§ Running config 784/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8835

ğŸ”§ Running config 785/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 786/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 787/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 788/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5550

ğŸ”§ Running config 789/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 790/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 791/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9365

ğŸ”§ Running config 792/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9142

ğŸ”§ Running config 793/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8952

ğŸ”§ Running config 794/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8836

ğŸ”§ Running config 795/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8817

ğŸ”§ Running config 796/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 797/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 798/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8804

ğŸ”§ Running config 799/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 800/1920: {'hidden_layers': [128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 801/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7466

ğŸ”§ Running config 802/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7891

ğŸ”§ Running config 803/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7102

ğŸ”§ Running config 804/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6954

ğŸ”§ Running config 805/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6313

ğŸ”§ Running config 806/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7543

ğŸ”§ Running config 807/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6333

ğŸ”§ Running config 808/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.5742

ğŸ”§ Running config 809/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.5734

ğŸ”§ Running config 810/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.5984

ğŸ”§ Running config 811/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8573

ğŸ”§ Running config 812/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9119

ğŸ”§ Running config 813/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7570

ğŸ”§ Running config 814/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7664

ğŸ”§ Running config 815/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8164

ğŸ”§ Running config 816/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7986

ğŸ”§ Running config 817/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7334

ğŸ”§ Running config 818/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6440

ğŸ”§ Running config 819/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7047

ğŸ”§ Running config 820/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7380

ğŸ”§ Running config 821/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7266

ğŸ”§ Running config 822/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6841

ğŸ”§ Running config 823/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6724

ğŸ”§ Running config 824/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6616

ğŸ”§ Running config 825/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6343

ğŸ”§ Running config 826/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5511

ğŸ”§ Running config 827/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.5687

ğŸ”§ Running config 828/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.5784

ğŸ”§ Running config 829/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.5657

ğŸ”§ Running config 830/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.5962

ğŸ”§ Running config 831/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8510

ğŸ”§ Running config 832/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8301

ğŸ”§ Running config 833/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7258

ğŸ”§ Running config 834/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7179

ğŸ”§ Running config 835/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8059

ğŸ”§ Running config 836/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5884

ğŸ”§ Running config 837/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6833

ğŸ”§ Running config 838/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6335

ğŸ”§ Running config 839/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7076

ğŸ”§ Running config 840/1920: {'hidden_layers': [256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7267

ğŸ”§ Running config 841/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4778

ğŸ”§ Running config 842/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4926

ğŸ”§ Running config 843/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4563

ğŸ”§ Running config 844/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3265

ğŸ”§ Running config 845/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1471

ğŸ”§ Running config 846/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2536

ğŸ”§ Running config 847/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7004

ğŸ”§ Running config 848/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2685

ğŸ”§ Running config 849/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0168

ğŸ”§ Running config 850/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0001

ğŸ”§ Running config 851/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7088

ğŸ”§ Running config 852/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4393

ğŸ”§ Running config 853/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4081

ğŸ”§ Running config 854/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1387

ğŸ”§ Running config 855/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0950

ğŸ”§ Running config 856/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3318

ğŸ”§ Running config 857/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5786

ğŸ”§ Running config 858/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2146

ğŸ”§ Running config 859/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0046

ğŸ”§ Running config 860/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0356

ğŸ”§ Running config 861/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1356

ğŸ”§ Running config 862/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4192

ğŸ”§ Running config 863/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1597

ğŸ”§ Running config 864/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4914

ğŸ”§ Running config 865/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0928

ğŸ”§ Running config 866/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9490

ğŸ”§ Running config 867/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0244

ğŸ”§ Running config 868/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0125

ğŸ”§ Running config 869/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0623

ğŸ”§ Running config 870/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8872

ğŸ”§ Running config 871/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3564

ğŸ”§ Running config 872/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2236

ğŸ”§ Running config 873/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2434

ğŸ”§ Running config 874/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2280

ğŸ”§ Running config 875/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0654

ğŸ”§ Running config 876/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1133

ğŸ”§ Running config 877/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9657

ğŸ”§ Running config 878/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0611

ğŸ”§ Running config 879/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0182

ğŸ”§ Running config 880/1920: {'hidden_layers': [256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9670

ğŸ”§ Running config 881/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8957

ğŸ”§ Running config 882/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6034

ğŸ”§ Running config 883/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8208

ğŸ”§ Running config 884/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0335

ğŸ”§ Running config 885/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.8620

ğŸ”§ Running config 886/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8877

ğŸ”§ Running config 887/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4152

ğŸ”§ Running config 888/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9428

ğŸ”§ Running config 889/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.7354

ğŸ”§ Running config 890/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4339

ğŸ”§ Running config 891/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8573

ğŸ”§ Running config 892/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8832

ğŸ”§ Running config 893/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5510

ğŸ”§ Running config 894/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3187

ğŸ”§ Running config 895/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1521

ğŸ”§ Running config 896/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8438

ğŸ”§ Running config 897/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 898/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3545

ğŸ”§ Running config 899/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1824

ğŸ”§ Running config 900/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0292

ğŸ”§ Running config 901/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8846

ğŸ”§ Running config 902/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7169

ğŸ”§ Running config 903/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2683

ğŸ”§ Running config 904/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8419

ğŸ”§ Running config 905/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5382

ğŸ”§ Running config 906/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 907/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1701

ğŸ”§ Running config 908/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9693

ğŸ”§ Running config 909/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.7752

ğŸ”§ Running config 910/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5340

ğŸ”§ Running config 911/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8846

ğŸ”§ Running config 912/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7339

ğŸ”§ Running config 913/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3853

ğŸ”§ Running config 914/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2998

ğŸ”§ Running config 915/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0813

ğŸ”§ Running config 916/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 917/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 918/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1226

ğŸ”§ Running config 919/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8508

ğŸ”§ Running config 920/1920: {'hidden_layers': [256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.7865

ğŸ”§ Running config 921/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8979

ğŸ”§ Running config 922/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 923/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6527

ğŸ”§ Running config 924/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8819

ğŸ”§ Running config 925/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6756

ğŸ”§ Running config 926/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 927/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 928/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6838

ğŸ”§ Running config 929/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8428

ğŸ”§ Running config 930/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5436

ğŸ”§ Running config 931/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8979

ğŸ”§ Running config 932/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 933/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8396

ğŸ”§ Running config 934/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8778

ğŸ”§ Running config 935/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6874

ğŸ”§ Running config 936/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 937/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 938/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7869

ğŸ”§ Running config 939/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 940/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5543

ğŸ”§ Running config 941/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8979

ğŸ”§ Running config 942/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 943/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6731

ğŸ”§ Running config 944/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8293

ğŸ”§ Running config 945/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4066

ğŸ”§ Running config 946/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 947/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 948/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4600

ğŸ”§ Running config 949/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0774

ğŸ”§ Running config 950/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4275

ğŸ”§ Running config 951/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8979

ğŸ”§ Running config 952/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 953/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8904

ğŸ”§ Running config 954/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 955/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5328

ğŸ”§ Running config 956/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 957/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 958/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8876

ğŸ”§ Running config 959/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 960/1920: {'hidden_layers': [256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2784

ğŸ”§ Running config 961/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2309

ğŸ”§ Running config 962/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0751

ğŸ”§ Running config 963/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9450

ğŸ”§ Running config 964/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8964

ğŸ”§ Running config 965/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9634

ğŸ”§ Running config 966/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0034

ğŸ”§ Running config 967/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9115

ğŸ”§ Running config 968/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9054

ğŸ”§ Running config 969/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9010

ğŸ”§ Running config 970/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9787

ğŸ”§ Running config 971/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0329

ğŸ”§ Running config 972/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1343

ğŸ”§ Running config 973/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1805

ğŸ”§ Running config 974/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1550

ğŸ”§ Running config 975/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0841

ğŸ”§ Running config 976/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0639

ğŸ”§ Running config 977/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0031

ğŸ”§ Running config 978/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9965

ğŸ”§ Running config 979/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1442

ğŸ”§ Running config 980/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2115

ğŸ”§ Running config 981/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9857

ğŸ”§ Running config 982/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0156

ğŸ”§ Running config 983/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9961

ğŸ”§ Running config 984/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8614

ğŸ”§ Running config 985/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9623

ğŸ”§ Running config 986/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7973

ğŸ”§ Running config 987/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8118

ğŸ”§ Running config 988/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8258

ğŸ”§ Running config 989/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8894

ğŸ”§ Running config 990/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9731

ğŸ”§ Running config 991/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9873

ğŸ”§ Running config 992/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1443

ğŸ”§ Running config 993/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1028

ğŸ”§ Running config 994/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1152

ğŸ”§ Running config 995/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0999

ğŸ”§ Running config 996/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9667

ğŸ”§ Running config 997/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0242

ğŸ”§ Running config 998/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0271

ğŸ”§ Running config 999/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0918

ğŸ”§ Running config 1000/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2259

ğŸ”§ Running config 1001/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4831

ğŸ”§ Running config 1002/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4937

ğŸ”§ Running config 1003/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5285

ğŸ”§ Running config 1004/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4573

ğŸ”§ Running config 1005/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0859

ğŸ”§ Running config 1006/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4266

ğŸ”§ Running config 1007/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3272

ğŸ”§ Running config 1008/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2229

ğŸ”§ Running config 1009/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3564

ğŸ”§ Running config 1010/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0126

ğŸ”§ Running config 1011/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8890

ğŸ”§ Running config 1012/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6734

ğŸ”§ Running config 1013/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6810

ğŸ”§ Running config 1014/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2533

ğŸ”§ Running config 1015/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2788

ğŸ”§ Running config 1016/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6120

ğŸ”§ Running config 1017/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5287

ğŸ”§ Running config 1018/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3821

ğŸ”§ Running config 1019/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3197

ğŸ”§ Running config 1020/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3191

ğŸ”§ Running config 1021/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2584

ğŸ”§ Running config 1022/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4002

ğŸ”§ Running config 1023/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1138

ğŸ”§ Running config 1024/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3351

ğŸ”§ Running config 1025/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2485

ğŸ”§ Running config 1026/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0756

ğŸ”§ Running config 1027/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2774

ğŸ”§ Running config 1028/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3357

ğŸ”§ Running config 1029/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1823

ğŸ”§ Running config 1030/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1888

ğŸ”§ Running config 1031/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5409

ğŸ”§ Running config 1032/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3269

ğŸ”§ Running config 1033/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3354

ğŸ”§ Running config 1034/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3678

ğŸ”§ Running config 1035/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4596

ğŸ”§ Running config 1036/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2664

ğŸ”§ Running config 1037/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3032

ğŸ”§ Running config 1038/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2537

ğŸ”§ Running config 1039/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3308

ğŸ”§ Running config 1040/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2287

ğŸ”§ Running config 1041/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8980

ğŸ”§ Running config 1042/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6696

ğŸ”§ Running config 1043/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8778

ğŸ”§ Running config 1044/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2812

ğŸ”§ Running config 1045/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1046/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8855

ğŸ”§ Running config 1047/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5371

ğŸ”§ Running config 1048/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1049/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3314

ğŸ”§ Running config 1050/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8801

ğŸ”§ Running config 1051/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8980

ğŸ”§ Running config 1052/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8710

ğŸ”§ Running config 1053/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8779

ğŸ”§ Running config 1054/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5682

ğŸ”§ Running config 1055/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 1056/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8855

ğŸ”§ Running config 1057/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8459

ğŸ”§ Running config 1058/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 1059/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5976

ğŸ”§ Running config 1060/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 1061/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8980

ğŸ”§ Running config 1062/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7346

ğŸ”§ Running config 1063/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8779

ğŸ”§ Running config 1064/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2629

ğŸ”§ Running config 1065/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 1066/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8855

ğŸ”§ Running config 1067/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4259

ğŸ”§ Running config 1068/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 1069/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0947

ğŸ”§ Running config 1070/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8801

ğŸ”§ Running config 1071/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8980

ğŸ”§ Running config 1072/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8866

ğŸ”§ Running config 1073/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8779

ğŸ”§ Running config 1074/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4483

ğŸ”§ Running config 1075/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1076/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8855

ğŸ”§ Running config 1077/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8811

ğŸ”§ Running config 1078/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8775

ğŸ”§ Running config 1079/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3030

ğŸ”§ Running config 1080/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 1081/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9003

ğŸ”§ Running config 1082/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9041

ğŸ”§ Running config 1083/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8928

ğŸ”§ Running config 1084/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1085/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8086

ğŸ”§ Running config 1086/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 1087/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8883

ğŸ”§ Running config 1088/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1089/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8800

ğŸ”§ Running config 1090/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8157

ğŸ”§ Running config 1091/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9003

ğŸ”§ Running config 1092/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9042

ğŸ”§ Running config 1093/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8900

ğŸ”§ Running config 1094/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 1095/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8830

ğŸ”§ Running config 1096/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 1097/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8884

ğŸ”§ Running config 1098/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8779

ğŸ”§ Running config 1099/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8801

ğŸ”§ Running config 1100/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1101/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9003

ğŸ”§ Running config 1102/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9041

ğŸ”§ Running config 1103/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8922

ğŸ”§ Running config 1104/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1105/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8822

ğŸ”§ Running config 1106/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 1107/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8883

ğŸ”§ Running config 1108/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1109/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8800

ğŸ”§ Running config 1110/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 1111/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9003

ğŸ”§ Running config 1112/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9042

ğŸ”§ Running config 1113/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8924

ğŸ”§ Running config 1114/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8781

ğŸ”§ Running config 1115/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8831

ğŸ”§ Running config 1116/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8889

ğŸ”§ Running config 1117/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8884

ğŸ”§ Running config 1118/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8845

ğŸ”§ Running config 1119/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8801

ğŸ”§ Running config 1120/1920: {'hidden_layers': [128, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1121/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1603

ğŸ”§ Running config 1122/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9796

ğŸ”§ Running config 1123/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8996

ğŸ”§ Running config 1124/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9358

ğŸ”§ Running config 1125/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8419

ğŸ”§ Running config 1126/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9934

ğŸ”§ Running config 1127/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8337

ğŸ”§ Running config 1128/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9128

ğŸ”§ Running config 1129/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8622

ğŸ”§ Running config 1130/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7937

ğŸ”§ Running config 1131/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0485

ğŸ”§ Running config 1132/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1871

ğŸ”§ Running config 1133/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9078

ğŸ”§ Running config 1134/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9823

ğŸ”§ Running config 1135/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0184

ğŸ”§ Running config 1136/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9025

ğŸ”§ Running config 1137/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9235

ğŸ”§ Running config 1138/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8775

ğŸ”§ Running config 1139/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8640

ğŸ”§ Running config 1140/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9976

ğŸ”§ Running config 1141/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0170

ğŸ”§ Running config 1142/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9369

ğŸ”§ Running config 1143/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8589

ğŸ”§ Running config 1144/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8701

ğŸ”§ Running config 1145/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8871

ğŸ”§ Running config 1146/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9193

ğŸ”§ Running config 1147/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8497

ğŸ”§ Running config 1148/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8813

ğŸ”§ Running config 1149/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8495

ğŸ”§ Running config 1150/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7734

ğŸ”§ Running config 1151/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9209

ğŸ”§ Running config 1152/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9222

ğŸ”§ Running config 1153/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8697

ğŸ”§ Running config 1154/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9670

ğŸ”§ Running config 1155/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9607

ğŸ”§ Running config 1156/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8694

ğŸ”§ Running config 1157/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9549

ğŸ”§ Running config 1158/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8684

ğŸ”§ Running config 1159/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8974

ğŸ”§ Running config 1160/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9675

ğŸ”§ Running config 1161/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7253

ğŸ”§ Running config 1162/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2666

ğŸ”§ Running config 1163/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3363

ğŸ”§ Running config 1164/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2561

ğŸ”§ Running config 1165/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1089

ğŸ”§ Running config 1166/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3646

ğŸ”§ Running config 1167/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2499

ğŸ”§ Running config 1168/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0988

ğŸ”§ Running config 1169/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3457

ğŸ”§ Running config 1170/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2242

ğŸ”§ Running config 1171/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.0898

ğŸ”§ Running config 1172/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6542

ğŸ”§ Running config 1173/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2978

ğŸ”§ Running config 1174/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3923

ğŸ”§ Running config 1175/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3004

ğŸ”§ Running config 1176/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5795

ğŸ”§ Running config 1177/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6098

ğŸ”§ Running config 1178/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2659

ğŸ”§ Running config 1179/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1976

ğŸ”§ Running config 1180/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2228

ğŸ”§ Running config 1181/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3323

ğŸ”§ Running config 1182/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4036

ğŸ”§ Running config 1183/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1497

ğŸ”§ Running config 1184/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3692

ğŸ”§ Running config 1185/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2529

ğŸ”§ Running config 1186/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1495

ğŸ”§ Running config 1187/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0282

ğŸ”§ Running config 1188/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3489

ğŸ”§ Running config 1189/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0957

ğŸ”§ Running config 1190/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2558

ğŸ”§ Running config 1191/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6875

ğŸ”§ Running config 1192/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5874

ğŸ”§ Running config 1193/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3123

ğŸ”§ Running config 1194/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3626

ğŸ”§ Running config 1195/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3550

ğŸ”§ Running config 1196/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3612

ğŸ”§ Running config 1197/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3561

ğŸ”§ Running config 1198/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3101

ğŸ”§ Running config 1199/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1276

ğŸ”§ Running config 1200/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2246

ğŸ”§ Running config 1201/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8926

ğŸ”§ Running config 1202/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8858

ğŸ”§ Running config 1203/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5718

ğŸ”§ Running config 1204/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1205/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4798

ğŸ”§ Running config 1206/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8836

ğŸ”§ Running config 1207/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1208/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5557

ğŸ”§ Running config 1209/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1210/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5969

ğŸ”§ Running config 1211/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8926

ğŸ”§ Running config 1212/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8857

ğŸ”§ Running config 1213/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1214/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1215/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8774

ğŸ”§ Running config 1216/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8836

ğŸ”§ Running config 1217/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1218/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 1219/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1220/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8795

ğŸ”§ Running config 1221/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8926

ğŸ”§ Running config 1222/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8858

ğŸ”§ Running config 1223/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8820

ğŸ”§ Running config 1224/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1225/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0482

ğŸ”§ Running config 1226/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8836

ğŸ”§ Running config 1227/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1228/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 1229/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1230/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0017

ğŸ”§ Running config 1231/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8926

ğŸ”§ Running config 1232/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8857

ğŸ”§ Running config 1233/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 1234/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1235/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8774

ğŸ”§ Running config 1236/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8836

ğŸ”§ Running config 1237/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1238/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6013

ğŸ”§ Running config 1239/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1240/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1241/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8967

ğŸ”§ Running config 1242/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8985

ğŸ”§ Running config 1243/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8908

ğŸ”§ Running config 1244/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8801

ğŸ”§ Running config 1245/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8749

ğŸ”§ Running config 1246/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1247/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8873

ğŸ”§ Running config 1248/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 1249/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 1250/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8518

ğŸ”§ Running config 1251/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8967

ğŸ”§ Running config 1252/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8986

ğŸ”§ Running config 1253/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8908

ğŸ”§ Running config 1254/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 1255/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1256/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1257/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8871

ğŸ”§ Running config 1258/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8805

ğŸ”§ Running config 1259/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8798

ğŸ”§ Running config 1260/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 1261/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8967

ğŸ”§ Running config 1262/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8986

ğŸ”§ Running config 1263/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8907

ğŸ”§ Running config 1264/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8783

ğŸ”§ Running config 1265/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5016

ğŸ”§ Running config 1266/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1267/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8872

ğŸ”§ Running config 1268/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 1269/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 1270/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5535

ğŸ”§ Running config 1271/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8967

ğŸ”§ Running config 1272/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8986

ğŸ”§ Running config 1273/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8907

ğŸ”§ Running config 1274/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 1275/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 1276/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1277/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8871

ğŸ”§ Running config 1278/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8805

ğŸ”§ Running config 1279/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 1280/1920: {'hidden_layers': [256, 128, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8776

ğŸ”§ Running config 1281/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9149

ğŸ”§ Running config 1282/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7635

ğŸ”§ Running config 1283/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8449

ğŸ”§ Running config 1284/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7712

ğŸ”§ Running config 1285/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6620

ğŸ”§ Running config 1286/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8009

ğŸ”§ Running config 1287/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7232

ğŸ”§ Running config 1288/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6714

ğŸ”§ Running config 1289/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6737

ğŸ”§ Running config 1290/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6068

ğŸ”§ Running config 1291/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1264

ğŸ”§ Running config 1292/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8438

ğŸ”§ Running config 1293/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7525

ğŸ”§ Running config 1294/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8174

ğŸ”§ Running config 1295/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7526

ğŸ”§ Running config 1296/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0596

ğŸ”§ Running config 1297/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8123

ğŸ”§ Running config 1298/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7293

ğŸ”§ Running config 1299/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6434

ğŸ”§ Running config 1300/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6805

ğŸ”§ Running config 1301/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7885

ğŸ”§ Running config 1302/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9018

ğŸ”§ Running config 1303/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7239

ğŸ”§ Running config 1304/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7279

ğŸ”§ Running config 1305/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7820

ğŸ”§ Running config 1306/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.5560

ğŸ”§ Running config 1307/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6954

ğŸ”§ Running config 1308/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6840

ğŸ”§ Running config 1309/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6582

ğŸ”§ Running config 1310/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.5378

ğŸ”§ Running config 1311/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8796

ğŸ”§ Running config 1312/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7622

ğŸ”§ Running config 1313/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8259

ğŸ”§ Running config 1314/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8051

ğŸ”§ Running config 1315/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7629

ğŸ”§ Running config 1316/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6926

ğŸ”§ Running config 1317/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7217

ğŸ”§ Running config 1318/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6987

ğŸ”§ Running config 1319/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6041

ğŸ”§ Running config 1320/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6657

ğŸ”§ Running config 1321/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8019

ğŸ”§ Running config 1322/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4402

ğŸ”§ Running config 1323/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1014

ğŸ”§ Running config 1324/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3998

ğŸ”§ Running config 1325/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1646

ğŸ”§ Running config 1326/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4369

ğŸ”§ Running config 1327/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3536

ğŸ”§ Running config 1328/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2938

ğŸ”§ Running config 1329/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2846

ğŸ”§ Running config 1330/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0583

ğŸ”§ Running config 1331/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9794

ğŸ”§ Running config 1332/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6524

ğŸ”§ Running config 1333/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3484

ğŸ”§ Running config 1334/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1901

ğŸ”§ Running config 1335/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3165

ğŸ”§ Running config 1336/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5403

ğŸ”§ Running config 1337/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4472

ğŸ”§ Running config 1338/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3182

ğŸ”§ Running config 1339/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2715

ğŸ”§ Running config 1340/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0966

ğŸ”§ Running config 1341/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3066

ğŸ”§ Running config 1342/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2643

ğŸ”§ Running config 1343/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4371

ğŸ”§ Running config 1344/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2032

ğŸ”§ Running config 1345/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3607

ğŸ”§ Running config 1346/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0441

ğŸ”§ Running config 1347/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1958

ğŸ”§ Running config 1348/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0462

ğŸ”§ Running config 1349/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0983

ğŸ”§ Running config 1350/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1646

ğŸ”§ Running config 1351/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4038

ğŸ”§ Running config 1352/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3587

ğŸ”§ Running config 1353/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4929

ğŸ”§ Running config 1354/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2635

ğŸ”§ Running config 1355/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9812

ğŸ”§ Running config 1356/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4346

ğŸ”§ Running config 1357/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1861

ğŸ”§ Running config 1358/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2064

ğŸ”§ Running config 1359/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1362

ğŸ”§ Running config 1360/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1621

ğŸ”§ Running config 1361/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1362/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8857

ğŸ”§ Running config 1363/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1364/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6997

ğŸ”§ Running config 1365/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3442

ğŸ”§ Running config 1366/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8840

ğŸ”§ Running config 1367/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 1368/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1369/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3661

ğŸ”§ Running config 1370/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3928

ğŸ”§ Running config 1371/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1372/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8858

ğŸ”§ Running config 1373/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7488

ğŸ”§ Running config 1374/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6816

ğŸ”§ Running config 1375/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4693

ğŸ”§ Running config 1376/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8841

ğŸ”§ Running config 1377/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1378/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6309

ğŸ”§ Running config 1379/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4900

ğŸ”§ Running config 1380/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4910

ğŸ”§ Running config 1381/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1382/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6798

ğŸ”§ Running config 1383/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8786

ğŸ”§ Running config 1384/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5890

ğŸ”§ Running config 1385/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2976

ğŸ”§ Running config 1386/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8841

ğŸ”§ Running config 1387/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5630

ğŸ”§ Running config 1388/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1389/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5334

ğŸ”§ Running config 1390/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2014

ğŸ”§ Running config 1391/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1392/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8858

ğŸ”§ Running config 1393/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7759

ğŸ”§ Running config 1394/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6265

ğŸ”§ Running config 1395/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5071

ğŸ”§ Running config 1396/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8841

ğŸ”§ Running config 1397/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8808

ğŸ”§ Running config 1398/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7080

ğŸ”§ Running config 1399/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4910

ğŸ”§ Running config 1400/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3903

ğŸ”§ Running config 1401/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9047

ğŸ”§ Running config 1402/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9016

ğŸ”§ Running config 1403/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8875

ğŸ”§ Running config 1404/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8771

ğŸ”§ Running config 1405/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8787

ğŸ”§ Running config 1406/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8944

ğŸ”§ Running config 1407/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8887

ğŸ”§ Running config 1408/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 1409/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1410/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7620

ğŸ”§ Running config 1411/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9047

ğŸ”§ Running config 1412/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9018

ğŸ”§ Running config 1413/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8880

ğŸ”§ Running config 1414/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 1415/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8778

ğŸ”§ Running config 1416/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8944

ğŸ”§ Running config 1417/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8887

ğŸ”§ Running config 1418/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 1419/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8360

ğŸ”§ Running config 1420/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8422

ğŸ”§ Running config 1421/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9036

ğŸ”§ Running config 1422/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9015

ğŸ”§ Running config 1423/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.9007

ğŸ”§ Running config 1424/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1425/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8798

ğŸ”§ Running config 1426/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8834

ğŸ”§ Running config 1427/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8887

ğŸ”§ Running config 1428/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 1429/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1430/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 1431/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9047

ğŸ”§ Running config 1432/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.9018

ğŸ”§ Running config 1433/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8995

ğŸ”§ Running config 1434/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8792

ğŸ”§ Running config 1435/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8804

ğŸ”§ Running config 1436/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8834

ğŸ”§ Running config 1437/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8887

ğŸ”§ Running config 1438/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8777

ğŸ”§ Running config 1439/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6925

ğŸ”§ Running config 1440/1920: {'hidden_layers': [512, 256, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 1441/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4341

ğŸ”§ Running config 1442/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0356

ğŸ”§ Running config 1443/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0791

ğŸ”§ Running config 1444/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0898

ğŸ”§ Running config 1445/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0170

ğŸ”§ Running config 1446/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1191

ğŸ”§ Running config 1447/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0154

ğŸ”§ Running config 1448/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1200

ğŸ”§ Running config 1449/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0615

ğŸ”§ Running config 1450/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2187

ğŸ”§ Running config 1451/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2773

ğŸ”§ Running config 1452/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4350

ğŸ”§ Running config 1453/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2614

ğŸ”§ Running config 1454/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5014

ğŸ”§ Running config 1455/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4270

ğŸ”§ Running config 1456/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2339

ğŸ”§ Running config 1457/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3960

ğŸ”§ Running config 1458/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4088

ğŸ”§ Running config 1459/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4496

ğŸ”§ Running config 1460/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5230

ğŸ”§ Running config 1461/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0375

ğŸ”§ Running config 1462/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1450

ğŸ”§ Running config 1463/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1941

ğŸ”§ Running config 1464/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1655

ğŸ”§ Running config 1465/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0546

ğŸ”§ Running config 1466/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0096

ğŸ”§ Running config 1467/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0706

ğŸ”§ Running config 1468/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9654

ğŸ”§ Running config 1469/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1153

ğŸ”§ Running config 1470/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2009

ğŸ”§ Running config 1471/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2380

ğŸ”§ Running config 1472/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2873

ğŸ”§ Running config 1473/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2658

ğŸ”§ Running config 1474/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4852

ğŸ”§ Running config 1475/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3256

ğŸ”§ Running config 1476/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0997

ğŸ”§ Running config 1477/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2383

ğŸ”§ Running config 1478/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2405

ğŸ”§ Running config 1479/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5497

ğŸ”§ Running config 1480/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.5551

ğŸ”§ Running config 1481/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6866

ğŸ”§ Running config 1482/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7914

ğŸ”§ Running config 1483/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3928

ğŸ”§ Running config 1484/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3690

ğŸ”§ Running config 1485/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1903

ğŸ”§ Running config 1486/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6158

ğŸ”§ Running config 1487/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5623

ğŸ”§ Running config 1488/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3737

ğŸ”§ Running config 1489/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3370

ğŸ”§ Running config 1490/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3282

ğŸ”§ Running config 1491/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9677

ğŸ”§ Running config 1492/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7778

ğŸ”§ Running config 1493/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5491

ğŸ”§ Running config 1494/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5131

ğŸ”§ Running config 1495/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3544

ğŸ”§ Running config 1496/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7624

ğŸ”§ Running config 1497/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7499

ğŸ”§ Running config 1498/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4419

ğŸ”§ Running config 1499/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4861

ğŸ”§ Running config 1500/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3805

ğŸ”§ Running config 1501/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3036

ğŸ”§ Running config 1502/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4316

ğŸ”§ Running config 1503/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2581

ğŸ”§ Running config 1504/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4577

ğŸ”§ Running config 1505/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4993

ğŸ”§ Running config 1506/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4093

ğŸ”§ Running config 1507/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3972

ğŸ”§ Running config 1508/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2883

ğŸ”§ Running config 1509/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2423

ğŸ”§ Running config 1510/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2775

ğŸ”§ Running config 1511/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5200

ğŸ”§ Running config 1512/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6176

ğŸ”§ Running config 1513/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6037

ğŸ”§ Running config 1514/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.5910

ğŸ”§ Running config 1515/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.4725

ğŸ”§ Running config 1516/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4562

ğŸ”§ Running config 1517/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5036

ğŸ”§ Running config 1518/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5247

ğŸ”§ Running config 1519/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4927

ğŸ”§ Running config 1520/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3250

ğŸ”§ Running config 1521/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8962

ğŸ”§ Running config 1522/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6772

ğŸ”§ Running config 1523/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4575

ğŸ”§ Running config 1524/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2662

ğŸ”§ Running config 1525/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3228

ğŸ”§ Running config 1526/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.6178

ğŸ”§ Running config 1527/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6198

ğŸ”§ Running config 1528/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.2243

ğŸ”§ Running config 1529/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2150

ğŸ”§ Running config 1530/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.2633

ğŸ”§ Running config 1531/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 1532/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8901

ğŸ”§ Running config 1533/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7215

ğŸ”§ Running config 1534/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8213

ğŸ”§ Running config 1535/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8683

ğŸ”§ Running config 1536/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8828

ğŸ”§ Running config 1537/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6322

ğŸ”§ Running config 1538/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6979

ğŸ”§ Running config 1539/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8498

ğŸ”§ Running config 1540/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5221

ğŸ”§ Running config 1541/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 1542/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6633

ğŸ”§ Running config 1543/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4042

ğŸ”§ Running config 1544/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1930

ğŸ”§ Running config 1545/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0656

ğŸ”§ Running config 1546/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8828

ğŸ”§ Running config 1547/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4242

ğŸ”§ Running config 1548/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1694

ğŸ”§ Running config 1549/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0913

ğŸ”§ Running config 1550/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1511

ğŸ”§ Running config 1551/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8854

ğŸ”§ Running config 1552/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7748

ğŸ”§ Running config 1553/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7297

ğŸ”§ Running config 1554/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 1555/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5569

ğŸ”§ Running config 1556/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8828

ğŸ”§ Running config 1557/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6974

ğŸ”§ Running config 1558/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6679

ğŸ”§ Running config 1559/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4904

ğŸ”§ Running config 1560/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1134

ğŸ”§ Running config 1561/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8963

ğŸ”§ Running config 1562/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8902

ğŸ”§ Running config 1563/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1564/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6853

ğŸ”§ Running config 1565/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7313

ğŸ”§ Running config 1566/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8830

ğŸ”§ Running config 1567/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8868

ğŸ”§ Running config 1568/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 1569/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7905

ğŸ”§ Running config 1570/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5312

ğŸ”§ Running config 1571/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8963

ğŸ”§ Running config 1572/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8902

ğŸ”§ Running config 1573/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8919

ğŸ”§ Running config 1574/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 1575/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8788

ğŸ”§ Running config 1576/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8830

ğŸ”§ Running config 1577/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8869

ğŸ”§ Running config 1578/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 1579/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 1580/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1581/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8963

ğŸ”§ Running config 1582/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8902

ğŸ”§ Running config 1583/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1584/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5359

ğŸ”§ Running config 1585/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8822

ğŸ”§ Running config 1586/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8830

ğŸ”§ Running config 1587/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8725

ğŸ”§ Running config 1588/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 1589/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.4705

ğŸ”§ Running config 1590/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5958

ğŸ”§ Running config 1591/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8963

ğŸ”§ Running config 1592/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8902

ğŸ”§ Running config 1593/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8918

ğŸ”§ Running config 1594/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8794

ğŸ”§ Running config 1595/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8812

ğŸ”§ Running config 1596/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8830

ğŸ”§ Running config 1597/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8869

ğŸ”§ Running config 1598/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8785

ğŸ”§ Running config 1599/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8784

ğŸ”§ Running config 1600/1920: {'hidden_layers': [64, 64, 64], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8202

ğŸ”§ Running config 1601/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0598

ğŸ”§ Running config 1602/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9269

ğŸ”§ Running config 1603/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0240

ğŸ”§ Running config 1604/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8931

ğŸ”§ Running config 1605/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7927

ğŸ”§ Running config 1606/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9772

ğŸ”§ Running config 1607/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8335

ğŸ”§ Running config 1608/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8045

ğŸ”§ Running config 1609/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7523

ğŸ”§ Running config 1610/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7121

ğŸ”§ Running config 1611/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1553

ğŸ”§ Running config 1612/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0457

ğŸ”§ Running config 1613/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0246

ğŸ”§ Running config 1614/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0212

ğŸ”§ Running config 1615/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0788

ğŸ”§ Running config 1616/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0923

ğŸ”§ Running config 1617/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9652

ğŸ”§ Running config 1618/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8769

ğŸ”§ Running config 1619/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8638

ğŸ”§ Running config 1620/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9715

ğŸ”§ Running config 1621/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9400

ğŸ”§ Running config 1622/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8907

ğŸ”§ Running config 1623/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9217

ğŸ”§ Running config 1624/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9182

ğŸ”§ Running config 1625/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8190

ğŸ”§ Running config 1626/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7869

ğŸ”§ Running config 1627/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7859

ğŸ”§ Running config 1628/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7977

ğŸ”§ Running config 1629/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7511

ğŸ”§ Running config 1630/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8348

ğŸ”§ Running config 1631/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9912

ğŸ”§ Running config 1632/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0476

ğŸ”§ Running config 1633/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0542

ğŸ”§ Running config 1634/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9816

ğŸ”§ Running config 1635/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0100

ğŸ”§ Running config 1636/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9736

ğŸ”§ Running config 1637/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8970

ğŸ”§ Running config 1638/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8892

ğŸ”§ Running config 1639/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8400

ğŸ”§ Running config 1640/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.9603

ğŸ”§ Running config 1641/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6675

ğŸ”§ Running config 1642/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4012

ğŸ”§ Running config 1643/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2907

ğŸ”§ Running config 1644/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1290

ğŸ”§ Running config 1645/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3997

ğŸ”§ Running config 1646/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5242

ğŸ”§ Running config 1647/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4096

ğŸ”§ Running config 1648/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2448

ğŸ”§ Running config 1649/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2665

ğŸ”§ Running config 1650/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0575

ğŸ”§ Running config 1651/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.8740

ğŸ”§ Running config 1652/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5692

ğŸ”§ Running config 1653/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4393

ğŸ”§ Running config 1654/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2463

ğŸ”§ Running config 1655/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2222

ğŸ”§ Running config 1656/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5929

ğŸ”§ Running config 1657/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3132

ğŸ”§ Running config 1658/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2514

ğŸ”§ Running config 1659/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1139

ğŸ”§ Running config 1660/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1349

ğŸ”§ Running config 1661/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2043

ğŸ”§ Running config 1662/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1566

ğŸ”§ Running config 1663/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3879

ğŸ”§ Running config 1664/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1806

ğŸ”§ Running config 1665/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2332

ğŸ”§ Running config 1666/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0457

ğŸ”§ Running config 1667/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2007

ğŸ”§ Running config 1668/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2174

ğŸ”§ Running config 1669/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.0966

ğŸ”§ Running config 1670/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1821

ğŸ”§ Running config 1671/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6370

ğŸ”§ Running config 1672/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3294

ğŸ”§ Running config 1673/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3703

ğŸ”§ Running config 1674/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1533

ğŸ”§ Running config 1675/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3087

ğŸ”§ Running config 1676/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.3050

ğŸ”§ Running config 1677/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2756

ğŸ”§ Running config 1678/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2967

ğŸ”§ Running config 1679/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2762

ğŸ”§ Running config 1680/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1085

ğŸ”§ Running config 1681/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8847

ğŸ”§ Running config 1682/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1683/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5905

ğŸ”§ Running config 1684/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1880

ğŸ”§ Running config 1685/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3042

ğŸ”§ Running config 1686/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1687/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4486

ğŸ”§ Running config 1688/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1302

ğŸ”§ Running config 1689/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8589

ğŸ”§ Running config 1690/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0691

ğŸ”§ Running config 1691/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8847

ğŸ”§ Running config 1692/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1693/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7017

ğŸ”§ Running config 1694/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5051

ğŸ”§ Running config 1695/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4300

ğŸ”§ Running config 1696/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1697/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8806

ğŸ”§ Running config 1698/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6742

ğŸ”§ Running config 1699/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3033

ğŸ”§ Running config 1700/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.3230

ğŸ”§ Running config 1701/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8847

ğŸ”§ Running config 1702/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8874

ğŸ”§ Running config 1703/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5352

ğŸ”§ Running config 1704/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.9523

ğŸ”§ Running config 1705/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0470

ğŸ”§ Running config 1706/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1707/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6162

ğŸ”§ Running config 1708/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4204

ğŸ”§ Running config 1709/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.8524

ğŸ”§ Running config 1710/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1974

ğŸ”§ Running config 1711/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8847

ğŸ”§ Running config 1712/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8865

ğŸ”§ Running config 1713/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7004

ğŸ”§ Running config 1714/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3718

ğŸ”§ Running config 1715/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8445

ğŸ”§ Running config 1716/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8799

ğŸ”§ Running config 1717/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8806

ğŸ”§ Running config 1718/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7151

ğŸ”§ Running config 1719/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.2161

ğŸ”§ Running config 1720/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5433

ğŸ”§ Running config 1721/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9197

ğŸ”§ Running config 1722/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1723/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8962

ğŸ”§ Running config 1724/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6886

ğŸ”§ Running config 1725/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.9191

ğŸ”§ Running config 1726/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1727/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 1728/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8820

ğŸ”§ Running config 1729/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6650

ğŸ”§ Running config 1730/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8779

ğŸ”§ Running config 1731/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9198

ğŸ”§ Running config 1732/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8917

ğŸ”§ Running config 1733/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8907

ğŸ”§ Running config 1734/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8752

ğŸ”§ Running config 1735/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8444

ğŸ”§ Running config 1736/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1737/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 1738/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8821

ğŸ”§ Running config 1739/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8436

ğŸ”§ Running config 1740/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6851

ğŸ”§ Running config 1741/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9198

ğŸ”§ Running config 1742/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8916

ğŸ”§ Running config 1743/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7727

ğŸ”§ Running config 1744/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8359

ğŸ”§ Running config 1745/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7073

ğŸ”§ Running config 1746/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1747/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.6412

ğŸ”§ Running config 1748/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6709

ğŸ”§ Running config 1749/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8813

ğŸ”§ Running config 1750/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6300

ğŸ”§ Running config 1751/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.9198

ğŸ”§ Running config 1752/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8917

ğŸ”§ Running config 1753/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8907

ğŸ”§ Running config 1754/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8820

ğŸ”§ Running config 1755/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8796

ğŸ”§ Running config 1756/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1757/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8790

ğŸ”§ Running config 1758/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8821

ğŸ”§ Running config 1759/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.7181

ğŸ”§ Running config 1760/1920: {'hidden_layers': [128, 128, 128], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1761/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8577

ğŸ”§ Running config 1762/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9710

ğŸ”§ Running config 1763/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8215

ğŸ”§ Running config 1764/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7992

ğŸ”§ Running config 1765/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7552

ğŸ”§ Running config 1766/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8906

ğŸ”§ Running config 1767/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7012

ğŸ”§ Running config 1768/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6670

ğŸ”§ Running config 1769/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6413

ğŸ”§ Running config 1770/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.5200

ğŸ”§ Running config 1771/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1915

ğŸ”§ Running config 1772/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0005

ğŸ”§ Running config 1773/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8687

ğŸ”§ Running config 1774/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.9066

ğŸ”§ Running config 1775/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7924

ğŸ”§ Running config 1776/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.9506

ğŸ”§ Running config 1777/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8326

ğŸ”§ Running config 1778/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7291

ğŸ”§ Running config 1779/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7675

ğŸ”§ Running config 1780/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7081

ğŸ”§ Running config 1781/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.8263

ğŸ”§ Running config 1782/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7787

ğŸ”§ Running config 1783/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8145

ğŸ”§ Running config 1784/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7697

ğŸ”§ Running config 1785/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.7797

ğŸ”§ Running config 1786/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.6917

ğŸ”§ Running config 1787/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6492

ğŸ”§ Running config 1788/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.6725

ğŸ”§ Running config 1789/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.6652

ğŸ”§ Running config 1790/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6533

ğŸ”§ Running config 1791/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7326

ğŸ”§ Running config 1792/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8603

ğŸ”§ Running config 1793/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8611

ğŸ”§ Running config 1794/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.8837

ğŸ”§ Running config 1795/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.8424

ğŸ”§ Running config 1796/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 1.7823

ğŸ”§ Running config 1797/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.7069

ğŸ”§ Running config 1798/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8292

ğŸ”§ Running config 1799/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 1.7719

ğŸ”§ Running config 1800/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 1.6898

ğŸ”§ Running config 1801/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.7456

ğŸ”§ Running config 1802/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4170

ğŸ”§ Running config 1803/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5015

ğŸ”§ Running config 1804/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4208

ğŸ”§ Running config 1805/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.3122

ğŸ”§ Running config 1806/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.5662

ğŸ”§ Running config 1807/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3897

ğŸ”§ Running config 1808/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3722

ğŸ”§ Running config 1809/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.4135

ğŸ”§ Running config 1810/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0936

ğŸ”§ Running config 1811/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.9305

ğŸ”§ Running config 1812/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6473

ğŸ”§ Running config 1813/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6173

ğŸ”§ Running config 1814/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2906

ğŸ”§ Running config 1815/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1403

ğŸ”§ Running config 1816/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.6769

ğŸ”§ Running config 1817/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4251

ğŸ”§ Running config 1818/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2939

ğŸ”§ Running config 1819/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3360

ğŸ”§ Running config 1820/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0544

ğŸ”§ Running config 1821/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.1863

ğŸ”§ Running config 1822/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1706

ğŸ”§ Running config 1823/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1377

ğŸ”§ Running config 1824/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.3281

ğŸ”§ Running config 1825/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.2203

ğŸ”§ Running config 1826/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.0285

ğŸ”§ Running config 1827/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3013

ğŸ”§ Running config 1828/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1863

ğŸ”§ Running config 1829/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2144

ğŸ”§ Running config 1830/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.0609

ğŸ”§ Running config 1831/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.4157

ğŸ”§ Running config 1832/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3725

ğŸ”§ Running config 1833/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3710

ğŸ”§ Running config 1834/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.2065

ğŸ”§ Running config 1835/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1245

ğŸ”§ Running config 1836/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 2.2893

ğŸ”§ Running config 1837/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2027

ğŸ”§ Running config 1838/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1152

ğŸ”§ Running config 1839/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 2.1135

ğŸ”§ Running config 1840/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.1588

ğŸ”§ Running config 1841/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8928

ğŸ”§ Running config 1842/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8843

ğŸ”§ Running config 1843/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6601

ğŸ”§ Running config 1844/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3117

ğŸ”§ Running config 1845/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.0923

ğŸ”§ Running config 1846/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1847/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1848/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.4379

ğŸ”§ Running config 1849/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0705

ğŸ”§ Running config 1850/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9932

ğŸ”§ Running config 1851/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8833

ğŸ”§ Running config 1852/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8815

ğŸ”§ Running config 1853/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7280

ğŸ”§ Running config 1854/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8376

ğŸ”§ Running config 1855/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6472

ğŸ”§ Running config 1856/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1857/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8782

ğŸ”§ Running config 1858/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7265

ğŸ”§ Running config 1859/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8317

ğŸ”§ Running config 1860/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4506

ğŸ”§ Running config 1861/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7480

ğŸ”§ Running config 1862/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.7286

ğŸ”§ Running config 1863/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5142

ğŸ”§ Running config 1864/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.1364

ğŸ”§ Running config 1865/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.1163

ğŸ”§ Running config 1866/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.7312

ğŸ”§ Running config 1867/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.5321

ğŸ”§ Running config 1868/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.5095

ğŸ”§ Running config 1869/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.0500

ğŸ”§ Running config 1870/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 2.9421

ğŸ”§ Running config 1871/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8928

ğŸ”§ Running config 1872/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8849

ğŸ”§ Running config 1873/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7408

ğŸ”§ Running config 1874/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.6638

ğŸ”§ Running config 1875/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5411

ğŸ”§ Running config 1876/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1877/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8810

ğŸ”§ Running config 1878/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6358

ğŸ”§ Running config 1879/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5820

ğŸ”§ Running config 1880/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.05, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4224

ğŸ”§ Running config 1881/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8924

ğŸ”§ Running config 1882/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8936

ğŸ”§ Running config 1883/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8876

ğŸ”§ Running config 1884/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8823

ğŸ”§ Running config 1885/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8774

ğŸ”§ Running config 1886/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1887/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8806

ğŸ”§ Running config 1888/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8868

ğŸ”§ Running config 1889/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8798

ğŸ”§ Running config 1890/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.8729

ğŸ”§ Running config 1891/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8924

ğŸ”§ Running config 1892/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8936

ğŸ”§ Running config 1893/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7940

ğŸ”§ Running config 1894/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8791

ğŸ”§ Running config 1895/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.7013

ğŸ”§ Running config 1896/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1897/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 1898/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7611

ğŸ”§ Running config 1899/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8802

ğŸ”§ Running config 1900/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.001, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6909

ğŸ”§ Running config 1901/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8924

ğŸ”§ Running config 1902/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8900

ğŸ”§ Running config 1903/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.7625

ğŸ”§ Running config 1904/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.5537

ğŸ”§ Running config 1905/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6146

ğŸ”§ Running config 1906/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1907/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8806

ğŸ”§ Running config 1908/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.6341

ğŸ”§ Running config 1909/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.3228

ğŸ”§ Running config 1910/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.5922

ğŸ”§ Running config 1911/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8925

ğŸ”§ Running config 1912/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8936

ğŸ”§ Running config 1913/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8908

ğŸ”§ Running config 1914/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8794

ğŸ”§ Running config 1915/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.6430

ğŸ”§ Running config 1916/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
âœ… Final Val Loss: 3.8809

ğŸ”§ Running config 1917/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.8807

ğŸ”§ Running config 1918/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.8793

ğŸ”§ Running config 1919/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 128}
âœ… Final Val Loss: 3.8789

ğŸ”§ Running config 1920/1920: {'hidden_layers': [256, 256, 256], 'lr': 0.1, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 256}
âœ… Final Val Loss: 3.4909

ğŸ† Best config: {'hidden_layers': [128], 'lr': 0.001, 'lambda_reg': 0.01, 'dropout': 0.1, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 16}
ğŸ“‰ Best validation loss: 1.5128
