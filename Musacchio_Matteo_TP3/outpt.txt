ğŸ” Probing 288 configurations...

ğŸ”§ Running config 1/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7116

ğŸ”§ Running config 2/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7168

ğŸ”§ Running config 3/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.3709

ğŸ”§ Running config 4/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7545

ğŸ”§ Running config 5/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7073

ğŸ”§ Running config 6/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8383

ğŸ”§ Running config 7/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1547

ğŸ”§ Running config 8/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7684

ğŸ”§ Running config 9/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.9801

ğŸ”§ Running config 10/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.4730

ğŸ”§ Running config 11/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5293

ğŸ”§ Running config 12/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4561

ğŸ”§ Running config 13/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0345

ğŸ”§ Running config 14/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0148

ğŸ”§ Running config 15/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.9535

ğŸ”§ Running config 16/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7250

ğŸ”§ Running config 17/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1014

ğŸ”§ Running config 18/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.1997

ğŸ”§ Running config 19/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.0069

ğŸ”§ Running config 20/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.8201

ğŸ”§ Running config 21/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4469

ğŸ”§ Running config 22/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.7958

ğŸ”§ Running config 23/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5516

ğŸ”§ Running config 24/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3736

ğŸ”§ Running config 25/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7033

ğŸ”§ Running config 26/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7160

ğŸ”§ Running config 27/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5988

ğŸ”§ Running config 28/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5263

ğŸ”§ Running config 29/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7042

ğŸ”§ Running config 30/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8391

ğŸ”§ Running config 31/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5195

ğŸ”§ Running config 32/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3274

ğŸ”§ Running config 33/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.9821

ğŸ”§ Running config 34/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.4740

ğŸ”§ Running config 35/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1524

ğŸ”§ Running config 36/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1628

ğŸ”§ Running config 37/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0295

ğŸ”§ Running config 38/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0138

ğŸ”§ Running config 39/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6570

ğŸ”§ Running config 40/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5278

ğŸ”§ Running config 41/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1007

ğŸ”§ Running config 42/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2001

ğŸ”§ Running config 43/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4063

ğŸ”§ Running config 44/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4361

ğŸ”§ Running config 45/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4477

ğŸ”§ Running config 46/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.7965

ğŸ”§ Running config 47/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3275

ğŸ”§ Running config 48/288: {'hidden_layers': [64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1654

ğŸ”§ Running config 49/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7186

ğŸ”§ Running config 50/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8767

ğŸ”§ Running config 51/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3859

ğŸ”§ Running config 52/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2513

ğŸ”§ Running config 53/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8115

ğŸ”§ Running config 54/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.1397

ğŸ”§ Running config 55/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4029

ğŸ”§ Running config 56/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2531

ğŸ”§ Running config 57/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4720

ğŸ”§ Running config 58/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.1403

ğŸ”§ Running config 59/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2040

ğŸ”§ Running config 60/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0032

ğŸ”§ Running config 61/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0151

ğŸ”§ Running config 62/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2433

ğŸ”§ Running config 63/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5993

ğŸ”§ Running config 64/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3859

ğŸ”§ Running config 65/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1660

ğŸ”§ Running config 66/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.5175

ğŸ”§ Running config 67/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5288

ğŸ”§ Running config 68/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3232

ğŸ”§ Running config 69/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.7951

ğŸ”§ Running config 70/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.3120

ğŸ”§ Running config 71/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3418

ğŸ”§ Running config 72/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1993

ğŸ”§ Running config 73/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7169

ğŸ”§ Running config 74/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8774

ğŸ”§ Running config 75/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2227

ğŸ”§ Running config 76/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0674

ğŸ”§ Running config 77/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8128

ğŸ”§ Running config 78/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.1406

ğŸ”§ Running config 79/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1368

ğŸ”§ Running config 80/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0647

ğŸ”§ Running config 81/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4741

ğŸ”§ Running config 82/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.1408

ğŸ”§ Running config 83/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9091

ğŸ”§ Running config 84/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8908

ğŸ”§ Running config 85/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0135

ğŸ”§ Running config 86/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2435

ğŸ”§ Running config 87/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2501

ğŸ”§ Running config 88/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3487

ğŸ”§ Running config 89/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1662

ğŸ”§ Running config 90/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.5182

ğŸ”§ Running config 91/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1618

ğŸ”§ Running config 92/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3691

ğŸ”§ Running config 93/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.7966

ğŸ”§ Running config 94/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.3123

ğŸ”§ Running config 95/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0331

ğŸ”§ Running config 96/288: {'hidden_layers': [64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1241

ğŸ”§ Running config 97/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6716

ğŸ”§ Running config 98/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.6702

ğŸ”§ Running config 99/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.4506

ğŸ”§ Running config 100/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2136

ğŸ”§ Running config 101/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6630

ğŸ”§ Running config 102/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7778

ğŸ”§ Running config 103/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.0295

ğŸ”§ Running config 104/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1466

ğŸ”§ Running config 105/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8952

ğŸ”§ Running config 106/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.3058

ğŸ”§ Running config 107/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4349

ğŸ”§ Running config 108/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0152

ğŸ”§ Running config 109/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8277

ğŸ”§ Running config 110/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8594

ğŸ”§ Running config 111/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6984

ğŸ”§ Running config 112/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.7238

ğŸ”§ Running config 113/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8249

ğŸ”§ Running config 114/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0772

ğŸ”§ Running config 115/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.8300

ğŸ”§ Running config 116/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4481

ğŸ”§ Running config 117/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1231

ğŸ”§ Running config 118/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.5425

ğŸ”§ Running config 119/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6669

ğŸ”§ Running config 120/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4868

ğŸ”§ Running config 121/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6628

ğŸ”§ Running config 122/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.6697

ğŸ”§ Running config 123/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.8175

ğŸ”§ Running config 124/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3042

ğŸ”§ Running config 125/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6607

ğŸ”§ Running config 126/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7784

ğŸ”§ Running config 127/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2655

ğŸ”§ Running config 128/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0703

ğŸ”§ Running config 129/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8972

ğŸ”§ Running config 130/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.3070

ğŸ”§ Running config 131/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8690

ğŸ”§ Running config 132/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8511

ğŸ”§ Running config 133/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8210

ğŸ”§ Running config 134/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8593

ğŸ”§ Running config 135/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4916

ğŸ”§ Running config 136/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.6179

ğŸ”§ Running config 137/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8262

ğŸ”§ Running config 138/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0773

ğŸ”§ Running config 139/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3167

ğŸ”§ Running config 140/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4032

ğŸ”§ Running config 141/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1246

ğŸ”§ Running config 142/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.5433

ğŸ”§ Running config 143/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1603

ğŸ”§ Running config 144/288: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2925

ğŸ”§ Running config 145/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6707

ğŸ”§ Running config 146/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8080

ğŸ”§ Running config 147/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0382

ğŸ”§ Running config 148/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0122

ğŸ”§ Running config 149/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7555

ğŸ”§ Running config 150/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0263

ğŸ”§ Running config 151/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0353

ğŸ”§ Running config 152/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9944

ğŸ”§ Running config 153/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.3060

ğŸ”§ Running config 154/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.9264

ğŸ”§ Running config 155/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9007

ğŸ”§ Running config 156/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8783

ğŸ”§ Running config 157/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8585

ğŸ”§ Running config 158/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0394

ğŸ”§ Running config 159/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4699

ğŸ”§ Running config 160/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2026

ğŸ”§ Running config 161/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0536

ğŸ”§ Running config 162/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2731

ğŸ”§ Running config 163/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5510

ğŸ”§ Running config 164/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1663

ğŸ”§ Running config 165/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.5426

ğŸ”§ Running config 166/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.0976

ğŸ”§ Running config 167/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1806

ğŸ”§ Running config 168/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9758

ğŸ”§ Running config 169/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6692

ğŸ”§ Running config 170/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.8090

ğŸ”§ Running config 171/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9103

ğŸ”§ Running config 172/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.8122

ğŸ”§ Running config 173/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7567

ğŸ”§ Running config 174/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0275

ğŸ”§ Running config 175/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.8787

ğŸ”§ Running config 176/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7983

ğŸ”§ Running config 177/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.3083

ğŸ”§ Running config 178/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.9272

ğŸ”§ Running config 179/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.6800

ğŸ”§ Running config 180/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.7124

ğŸ”§ Running config 181/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8575

ğŸ”§ Running config 182/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0399

ğŸ”§ Running config 183/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0780

ğŸ”§ Running config 184/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1430

ğŸ”§ Running config 185/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0544

ğŸ”§ Running config 186/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2738

ğŸ”§ Running config 187/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0726

ğŸ”§ Running config 188/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0162

ğŸ”§ Running config 189/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.5441

ğŸ”§ Running config 190/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.0981

ğŸ”§ Running config 191/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 1.9808

ğŸ”§ Running config 192/288: {'hidden_layers': [128], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 1.9524

ğŸ”§ Running config 193/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6954

ğŸ”§ Running config 194/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.6948

ğŸ”§ Running config 195/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1833

ğŸ”§ Running config 196/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.1409

ğŸ”§ Running config 197/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7057

ğŸ”§ Running config 198/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7343

ğŸ”§ Running config 199/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.3162

ğŸ”§ Running config 200/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 3.3985

ğŸ”§ Running config 201/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8263

ğŸ”§ Running config 202/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2686

ğŸ”§ Running config 203/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.1062

ğŸ”§ Running config 204/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5664

ğŸ”§ Running config 205/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.9938

ğŸ”§ Running config 206/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0796

ğŸ”§ Running config 207/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7365

ğŸ”§ Running config 208/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5771

ğŸ”§ Running config 209/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0593

ğŸ”§ Running config 210/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2966

ğŸ”§ Running config 211/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6448

ğŸ”§ Running config 212/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.4144

ğŸ”§ Running config 213/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4564

ğŸ”§ Running config 214/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.9521

ğŸ”§ Running config 215/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6608

ğŸ”§ Running config 216/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5375

ğŸ”§ Running config 217/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6826

ğŸ”§ Running config 218/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7068

ğŸ”§ Running config 219/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4708

ğŸ”§ Running config 220/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.8590

ğŸ”§ Running config 221/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6961

ğŸ”§ Running config 222/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7337

ğŸ”§ Running config 223/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.6114

ğŸ”§ Running config 224/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.9334

ğŸ”§ Running config 225/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.8283

ğŸ”§ Running config 226/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2699

ğŸ”§ Running config 227/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3881

ğŸ”§ Running config 228/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3160

ğŸ”§ Running config 229/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.9834

ğŸ”§ Running config 230/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.0774

ğŸ”§ Running config 231/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5079

ğŸ”§ Running config 232/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3298

ğŸ”§ Running config 233/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.0559

ğŸ”§ Running config 234/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.2966

ğŸ”§ Running config 235/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3625

ğŸ”§ Running config 236/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3686

ğŸ”§ Running config 237/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.4580

ğŸ”§ Running config 238/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.9519

ğŸ”§ Running config 239/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1277

ğŸ”§ Running config 240/288: {'hidden_layers': [128, 64], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3255

ğŸ”§ Running config 241/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.6939

ğŸ”§ Running config 242/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7629

ğŸ”§ Running config 243/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 3.2576

ğŸ”§ Running config 244/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5218

ğŸ”§ Running config 245/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7190

ğŸ”§ Running config 246/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.9527

ğŸ”§ Running config 247/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.7156

ğŸ”§ Running config 248/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3932

ğŸ”§ Running config 249/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.2670

ğŸ”§ Running config 250/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.0207

ğŸ”§ Running config 251/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.4565

ğŸ”§ Running config 252/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1700

ğŸ”§ Running config 253/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1269

ğŸ”§ Running config 254/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.3546

ğŸ”§ Running config 255/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3545

ğŸ”§ Running config 256/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2635

ğŸ”§ Running config 257/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.2954

ğŸ”§ Running config 258/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.6887

ğŸ”§ Running config 259/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5045

ğŸ”§ Running config 260/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2968

ğŸ”§ Running config 261/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.9628

ğŸ”§ Running config 262/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.4590

ğŸ”§ Running config 263/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2290

ğŸ”§ Running config 264/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.0, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.2216

ğŸ”§ Running config 265/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7048

ğŸ”§ Running config 266/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.7632

ğŸ”§ Running config 267/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3757

ğŸ”§ Running config 268/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3861

ğŸ”§ Running config 269/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 1.7190

ğŸ”§ Running config 270/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 1.9539

ğŸ”§ Running config 271/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.5308

ğŸ”§ Running config 272/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.3638

ğŸ”§ Running config 273/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.2702

ğŸ”§ Running config 274/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.0216

ğŸ”§ Running config 275/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.0660

ğŸ”§ Running config 276/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0257

ğŸ”§ Running config 277/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.1230

ğŸ”§ Running config 278/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.3548

ğŸ”§ Running config 279/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.3770

ğŸ”§ Running config 280/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': None, 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.5143

ğŸ”§ Running config 281/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.2949

ğŸ”§ Running config 282/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 2.6896

ğŸ”§ Running config 283/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.2708

ğŸ”§ Running config 284/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'linear', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.1824

ğŸ”§ Running config 285/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 32}
âœ… Final Val Loss: 2.9629

ğŸ”§ Running config 286/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': False, 'batch_size': 64}
âœ… Final Val Loss: 3.4592

ğŸ”§ Running config 287/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 32}
âœ… Final Val Loss: 2.1382

ğŸ”§ Running config 288/288: {'hidden_layers': [128, 64], 'lr': 0.005, 'lambda_reg': 0.01, 'dropout': 0.2, 'scheduler': 'exponential', 'use_adam': True, 'batch_size': 64}
âœ… Final Val Loss: 2.0491

ğŸ† Best config: {'hidden_layers': [128], 'lr': 0.01, 'lambda_reg': 0.01, 'dropout': 0.0, 'scheduler': 'linear', 'use_adam': False, 'batch_size': 32}
ğŸ“‰ Best validation loss: 1.6607
